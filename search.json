[
  {
    "objectID": "exercises/stats.html",
    "href": "exercises/stats.html",
    "title": "Statistics",
    "section": "",
    "text": "Check out the slides below or click here to view the slides in full screen.\n\n\n\nIn this third practical tutorial, we will learn to conduct statistical analysis with .\nTwo topics will be deepen in this tutorial: 1. Data aggregation and descriptive statistics 2. Inferential statistics, illustrated through the use case of survey data\nRather than providing a formal overview of classical inferential statistical techniques, this tutorial adopts an applied approach. We will explore these methods through a concrete and widely used case in official statistics: survey data analysis.\n\n\n\n\n\n\nNote\n\n\n\nAs you have already computed your first descriptive statistics in the previous practical session, this session will be less guided than usual. The objective is to give you more autonomy and allow you to practice your newly acquired skills in a different context.\n\n\n\n1 Warm-up: basic statistical analysis :running:\nLet’s begin gradually. The exercises will become progressively more technical, but we will move forward step by step.\n\n\n\n\n\n\nExercise 1: Set up your environment\n\n\n\nInstall and load the packages dplyr, tidyr, and stringr.\n\n\nHint (click to expand)\n\nIf you do not remember how to proceed, refer to the preliminary steps section from the previous session.\n\n\n\n\n\n\n\n\n\nExercise 2: Import data\n\n\n\nHere’s the URL where the data is available\nurl &lt;- “https://raw.githubusercontent.com/zief0002/miniature-garbanzo/main/data/gapminder.csv”\nVariable definitions: To understand each column and its meaning, check out the Gapminder codebook\nRead the data into R, display it, and get a first overview of its structure and contents.\n\n\nHint (click to expand)\n\nIf needed, refer to the importing data section from the last practical session.\n\n\n\n\n\n\n\n\n\nExercise 3: EDA - quick data cleaning\n\n\n\nExplore the Gapminder dataset carefully and prepare it for descriptive analysis. Focus on data quality, potential inconsistencies, and useful derived variables.\nConsider questions such as:\n\nAre the column types appropriate? Do any numeric columns need to be converted, or factor/character columns recoded?\nAre there missing values or unexpected duplicates? How would you handle them?\nAre there variables that could be transformed or combined to create new indicators (e.g., per capita measures, ratios, log-transformations)?\nAre there outliers or extreme values that should be noted before further analysis?\nSummarize your observations in a few lines: e.g., key statistics, potential issues, and any transformations you applied.\n\n\n\nHint (click to expand)\n\nUseful functions include glimpse(), str(), summary(), n_distinct(), duplicated(), mutate(), and filter(). Keep your exploration focused and concise aim for about 15–20 minutes of work.\n\n\n\nSolution question 3: an example of quick EDA and cleaning\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(readr)\n\n# 1. Import data\nurl &lt;- \"https://raw.githubusercontent.com/zief0002/miniature-garbanzo/main/data/gapminder.csv\"\ngapminder &lt;- read_csv(url)\n\n# 2. Inspect structure\nglimpse(gapminder)\nstr(gapminder)\nsummary(gapminder)\n\n# 3. Check duplicates and missing values\n# Are there any duplicate rows?\nsum(duplicated(gapminder))\n\n# Missing values per column\ncolSums(is.na(gapminder))\n\n# 4. Number of unique values in key columns\nn_distinct(gapminder$country)\nn_distinct(gapminder$region)\nn_distinct(gapminder$income_level)\nn_distinct(gapminder$year)  # if year column exists\n\n# 5. Create some derived variables\ngapminder &lt;- gapminder %&gt;%\n  mutate(\n    co2_per_capita = co2 / population,  # emissions per person\n    log_income = log(income)            # log transformation of income\n  )\n\n# 6. Quick descriptive summary\ngapminder %&gt;%\n  summarise(\n    total_countries = n_distinct(country),\n    mean_income = mean(income, na.rm = TRUE),\n    median_income = median(income, na.rm = TRUE),\n    mean_co2 = mean(co2, na.rm = TRUE),\n    mean_co2_pc = mean(co2_per_capita, na.rm = TRUE),\n    max_life_exp = max(life_exp, na.rm = TRUE)\n  )\n\n# 7. Optional: Inspect top countries by CO2 per capita\ngapminder %&gt;%\n  arrange(desc(co2_per_capita)) %&gt;%\n  select(country, region, co2_per_capita, population) %&gt;%\n  head(10)\n\n# 8. Optional: Look at regions with highest average income\ngapminder %&gt;%\n  group_by(region) %&gt;%\n  summarise(mean_income = mean(income, na.rm = TRUE)) %&gt;%\n  arrange(desc(mean_income))\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nUnlike the very similar exercise from the previous session, there is no single “correct” solution. What matters most here is the methodology, the reasoning, and your ability to use the tools (dplyr, tidyr, etc.) effectively.\n\n\n\n\n\n\n\n\nExercise 4.a – CO₂ and income by region\n\n\n\nExplore how average income and CO₂ emissions per capita vary across regions.\nConsider questions such as: 1. Which regions have high income but low emissions, or low income but high emissions? 2. Are there regional patterns in life expectancy and population that align with income and CO₂? 3. What might these patterns imply about development, energy use, and environmental impact?\n\n\nHint (click to expand)\n\nUse group_by(region) and summarise(); create derived variables like co2_per_capita and log_income for better comparison.\n\n\n\nSolution exercise 4.a: CO₂ and income by region\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(readr)\n\n# Import Gapminder dataset\nurl &lt;- \"https://raw.githubusercontent.com/zief0002/miniature-garbanzo/main/data/gapminder.csv\"\ngapminder &lt;- read_csv(url)\n\n# Create derived variables\ngapminder &lt;- gapminder %&gt;%\n  mutate(\n    co2_per_capita = co2 / population,\n    log_income = log(income)\n  )\n\n# Aggregate by region\nregion_summary &lt;- gapminder %&gt;%\n  group_by(region) %&gt;%\n  summarise(\n    mean_income = mean(income, na.rm = TRUE),\n    mean_co2_pc = mean(co2_per_capita, na.rm = TRUE),\n    mean_life_exp = mean(life_exp, na.rm = TRUE),\n    total_population = sum(population, na.rm = TRUE)\n  ) %&gt;%\n  arrange(desc(mean_income))\n\nregion_summary\n\n\n\n\n\n\n\n\n\n\nExercise 4.b – population and income levels\n\n\n\nInvestigate the relationship between population size and income levels.\nQuestions to explore: 1. Identify the most populous countries in each income level. 2. Compare their average income and life expectancy. 3. Are there countries with high population but low income, or high income but small population? 4. What insights can you draw about development challenges or resource allocation?\n\n\nHint (click to expand)\n\nCombine group_by(income_level), arrange(desc(population)), filter() and summarise() to explore patterns.\n\n\n\nSolution Exercise 4.b: Population and Income Levels\n# Most populous countries per income level\npop_by_income &lt;- gapminder %&gt;%\n  group_by(income_level) %&gt;%\n  arrange(desc(population)) %&gt;%\n  slice_head(n = 5) %&gt;%  # top 5 per income level\n  select(country, population, income, life_exp)\n\npop_by_income\n\n\n\n\n\n\n\n\n\n\nExercise 4.c – Outliers and cross-country comparisons\n\n\n\nFocus on outliers within regions or income levels.\nConsider: 1. Which countries have extremely high or low CO₂ per capita for their income level? 2. Are there countries with similar income but very different emissions, or similar emissions but very different populations? 3. How could these differences be explained (e.g., industrialization, geography, energy sources)?\n\n\nHint (click to expand)\n\nLook for arrange(desc(co2_per_capita)), filter() and compare subsets of countries.\n\n\n\nSolution Exercise 4.c: Outliers and Cross-Country Comparisons\n# Outliers in CO₂ per capita within income levels\noutliers &lt;- gapminder %&gt;%\n  group_by(income_level) %&gt;%\n  filter(co2_per_capita == max(co2_per_capita) | co2_per_capita == min(co2_per_capita)) %&gt;%\n  select(country, income_level, co2_per_capita, income, population)\n\noutliers\n\n\n\n\n\n\n\n\n\n\nExercise 4.d – Correlations and relationships by region\n\n\n\nExplore relationships between income, CO₂ emissions, life expectancy, and population within each region.\nQuestions to explore: 1. Are correlations between income and CO₂ emissions consistent across regions? 2. Are wealthier regions always emitting more CO₂ per capita? Are there exceptions? 3. How does population size relate to income and CO₂ within regions?\n\n\nHint (click to expand)\n\nUse group_by(region) %&gt;% summarise() for aggregate stats, and visualize with ggplot2 to spot trends and correlations.\n\n\n\nSolution Exercise 4.d: Correlations and Relationships by Region\n# Simple correlations per region\nregion_corr &lt;- gapminder %&gt;%\n  group_by(region) %&gt;%\n  summarise(\n    corr_income_co2 = cor(income, co2_per_capita, use = \"complete.obs\"),\n    corr_income_life = cor(income, life_exp, use = \"complete.obs\"),\n    corr_co2_life = cor(co2_per_capita, life_exp, use = \"complete.obs\")\n  )\n\nregion_corr\n\n# Optional visualization: scatter plot of CO₂ per capita vs income\nggplot(gapminder, aes(x = income, y = co2_per_capita, color = region)) +\n  geom_point(alpha = 0.7) +\n  scale_x_log10() + # log scale for better spread\n  labs(x = \"Income (log scale)\", y = \"CO₂ per capita\", title = \"CO₂ vs Income by Region\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\nExercise 5 – Finding “Champions” by Domain\n\n\n\nLet’s identify the countries or regions that stand out in different dimensions. For example, Africa has some of the largest populations; which countries lead in CO₂ emissions, income, or life expectancy?\nQuestions to explore:\n\nReshape and aggregate:\n\n\nConvert the Gapminder dataset to a long format for CO₂ emissions, income, and life expectancy, keeping country and region as the analysis level.\nAggregate or sum the values by variable of interest for each country or region.\n\n\nGraphical representation:\n\n\nProduce a barplot or other visualizations to identify the top performers in each domain (population, CO₂ per capita, income, life expectancy).\n\n\nChampion identification:\n\n\nFor each region, identify the country with the highest value in each key metric (CO₂ per capita, income, life expectancy, population).\nCompare patterns across regions: do some regions excel in certain dimensions while lagging in others?\n\n\n\nHint (click to expand)\n\nUse pivot_longer() to convert the dataset into long format if you want a single variable column for CO₂, income, and life expectancy.\nUse group_by(country, variable) and summarise() to calculate totals or averages.\nUse arrange(desc(value)) to rank countries within each variable or region.\nVisualize with ggplot2 using geom_bar(stat=“identity”) or geom_col().\nYou can also create a summary table of champions per region using slice_max() or top_n().\n\n\n\n\n\n\n\n\n\nExercise 5 Bonus – Identify the least polluting region\n\n\n\nNow that you have explored “champions” at the country level, let’s look at regions.\nTask: - Aggregate CO₂ emissions per region. You can use either total CO₂ or CO₂ per capita. - Identify the region with the lowest pollution.\nOptionally, visualize all regions to see how they compare.\nInterpret the results: does the least polluting region have a small population, low income, or other characteristics that might explain its low emissions?\n\n\nSolution Exercise 5 Bonus: Least Polluting Region\nlibrary(dplyr)\nlibrary(readr)\nlibrary(ggplot2)\n\n# Import Gapminder dataset\nurl &lt;- \"https://raw.githubusercontent.com/zief0002/miniature-garbanzo/main/data/gapminder.csv\"\ngapminder &lt;- read_csv(url)\n\n# Derived CO₂ metrics\ngapminder &lt;- gapminder %&gt;%\n  mutate(\n    co2_per_capita = co2 / population,\n    co2_total = co2_per_capita * population\n  )\n\n# Aggregate by region\nregion_co2 &lt;- gapminder %&gt;%\n  group_by(region) %&gt;%\n  summarise(\n    total_co2 = sum(co2_total, na.rm = TRUE),\n    mean_co2_pc = mean(co2_per_capita, na.rm = TRUE)\n  ) %&gt;%\n  arrange(mean_co2_pc)\n\nregion_co2\n\n# Least polluting region\nleast_polluting_region &lt;- region_co2 %&gt;%\n  slice_min(order_by = mean_co2_pc, n = 1)\n\nleast_polluting_region\n\n# Optional visualization\nggplot(region_co2, aes(x = reorder(region, mean_co2_pc), y = mean_co2_pc, fill = region)) +\n  geom_col(show.legend = FALSE) +\n  coord_flip() +\n  labs(\n    title = \"Average CO₂ per Capita by Region\",\n    x = \"Region\",\n    y = \"Mean CO₂ per Capita\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\nExercise 6 (optional) – Validate interpretations with statistical tests\n\n\n\nFor those who want to go further, test some of the patterns you observed in previous exercises.\nTask suggestions:\n\nIncome vs CO₂ per capita: Do high-income countries emit more CO₂ per capita than low-income countries?\nLife expectancy vs income level: Are wealthier countries living longer?\nCorrelations: Check whether CO₂ per capita is correlated with income or population.\n\n\n\nHints (click to expand)\n\n\nGroup countries by income_level or region.\n\nVisualize distributions with boxplots.\n\nChoose parametric (t-test, ANOVA) or non-parametric (Wilcoxon, Kruskal-Wallis) tests based on distribution.\n\nUse cor.test() to check correlations.\n\nInterpret p-values and confidence intervals in context.\n\n\n\n\nSolution Exercise 6 (optional): Statistical tests\nlibrary(dplyr)\nlibrary(readr)\nlibrary(ggplot2)\n\n# Import Gapminder dataset\nurl &lt;- \"https://raw.githubusercontent.com/zief0002/miniature-garbanzo/main/data/gapminder.csv\"\ngap &lt;- read_csv(url) %&gt;%\n  mutate(\n    co2_per_capita = co2 / population\n  )\n\n# Example 1: CO₂ per capita by income_level\ngap %&gt;%\n  group_by(income_level) %&gt;%\n  summarise(\n    mean_co2_pc = mean(co2_per_capita, na.rm = TRUE),\n    sd_co2_pc = sd(co2_per_capita, na.rm = TRUE)\n  )\n\n# Visual inspection\nggplot(gap, aes(x = income_level, y = co2_per_capita)) +\n  geom_boxplot() +\n  labs(title = \"CO₂ per Capita by Income Level\") +\n  theme_minimal()\n\n\nSolution Exercise 6 (optional): Statistical tests\n# T-test between Level 1 and Level 4 income countries (example)\nt_test_result &lt;- t.test(\n  co2_per_capita ~ income_level,\n  data = gap %&gt;% filter(income_level %in% c(\"Level 1\",\"Level 4\"))\n)\nt_test_result\n\n# Example 2: Correlation between income and CO₂ per capita\ncor_test &lt;- cor.test(gap$income, gap$co2_per_capita, method = \"spearman\")\ncor_test\n\n# Example 3: ANOVA for CO₂ per capita across all income levels\nanova_result &lt;- aov(co2_per_capita ~ income_level, data = gap)\nsummary(anova_result)\n\n\n\n\n\n\n2 Conducting a survey analysis :weight_lifting:\nIn this section, the goal is not to master every survey package in R.\nInstead, we focus on exploring some specific contexts and getting comfortable with the Hmisc library, which allows us to compute weighted statistics easily. Once you have the fundamentals, you’ll be able to explore documentation on your own and experiment with the many tools available. :mortar_board: :rocket:\n\n\n\n\n\n\nExercise 1 – Simulate a partial survey\n\n\n\n\nFrom the Gapminder dataset, randomly remove ~30% of CO₂ values to simulate a survey with missing data.\nAdd a weight variable proportional to the population.\nInspect the dataset to check the missing values and verify your weights.\n\n\n\nHint\n\n\nUse sample() to randomly select rows to set as NA.\nCheck missing values with colSums(is.na(…)).\nCreate weights with mutate(weight = population / sum(population, na.rm = TRUE)).\nUse str() or glimpse() to verify your table structure.\n\n\n\n\nSolution Exercise 1: Simulate survey and weights\nlibrary(dplyr)\nlibrary(readr)\n\n# Load Gapminder\nurl &lt;- \"https://raw.githubusercontent.com/zief0002/miniature-garbanzo/main/data/gapminder.csv\"\nsurvey_data &lt;- read_csv(url)\n\n# Simulate missing CO₂ values (~30%)\nset.seed(123)\nsurvey_data$co2[sample(1:nrow(survey_data), 30)] &lt;- NA\n\n# Add weights proportional to population\nsurvey_data &lt;- survey_data %&gt;%\n  mutate(weight = population / sum(population, na.rm = TRUE))\n\n# Inspect\ncolSums(is.na(survey_data))\nstr(survey_data)\n\n\n\n\n\n\n\n\n\n\nExercise 2 – Weighted average CO₂ per region\n\n\n\n\nCompute the weighted mean CO₂ per capita per region using Hmisc::wtd.mean.\nCompare weighted vs unweighted means.\n\n\n\nHint\n\n\nUse group_by(region) + summarise().\nUse wtd.mean(co2_per_capita, weights = weight, na.rm = TRUE) for weighted mean.\nFor unweighted mean, simply mean(co2_per_capita, na.rm = TRUE).\n\n\n\n\nSolution Exercise 2: Weighted CO2 per region\nlibrary(Hmisc)\n\nsurvey_data &lt;- survey_data %&gt;%\n  mutate(co2_per_capita = co2 / population)\n\nweighted_region &lt;- survey_data %&gt;%\n  group_by(region) %&gt;%\n  summarise(\n    weighted_mean = wtd.mean(co2_per_capita, weights = weight, na.rm = TRUE),\n    unweighted_mean = mean(co2_per_capita, na.rm = TRUE)\n  )\n\nweighted_region\n\n\n\n\n\n\n\n\n\n\nExercise 3 – Median CO₂ per income level\n\n\n\n\nCalculate the weighted median CO₂ per capita by income_level.\nCompare weighted median with the unweighted median.\n\n\n\nHint\n\n\nHmisc::wtd.quantile() can compute weighted medians (probs = 0.5).\nUse group_by(income_level) + summarise().\nRemember na.rm = TRUE for missing values.\n\n\n\n\nSolution Exercise 3: Weighted median by income level\nmedian_income &lt;- survey_data %&gt;%\n  group_by(income_level) %&gt;%\n  summarise(\n    weighted_median = wtd.quantile(co2_per_capita, weights = weight, probs = 0.5, na.rm = TRUE),\n    unweighted_median = median(co2_per_capita, na.rm = TRUE)\n  )\n\nmedian_income\n\n\n\n\n\n\n\n\n\n\nExercise 4 – Bootstrap estimation of mean CO₂\n\n\n\n\nPerform a simple bootstrap on the survey sample to estimate the variability of weighted mean CO₂ per region.\nRepeat 100 times, then summarize the mean ± standard deviation.\n\n\n\nHint\n\n\nUse sample_n(…, replace = TRUE) to resample rows.\nCompute weighted mean for each resample.\nSummarize results with mean() and sd().\n\n\n\n\nSolution Exercise 4: Bootstrap weighted mean CO2\nset.seed(123)\nlibrary(purrr)\n\nbootstrap_means &lt;- map_dbl(1:100, ~{\n  sample_data &lt;- survey_data %&gt;% sample_n(nrow(survey_data), replace = TRUE)\n  wtd.mean(sample_data$co2_per_capita, weights = sample_data$weight, na.rm = TRUE)\n})\n\nmean_boot &lt;- mean(bootstrap_means)\nsd_boot &lt;- sd(bootstrap_means)\n\nmean_boot\nsd_boot\n\n\n\n\n\n\n\n\n\n\nExercise 5 (Bonus) – Least polluting region with survey weights\n\n\n\n\nUsing your weighted survey data, identify the region with the lowest weighted CO₂ per capita.\nVisualize all regions with a barplot and highlight the least polluting one.\n\n\n\nHint\n\n\nAggregate by region with summarise().\nUse wtd.mean() for weighted averages.\nUse slice_min() to select the region with lowest mean.\nUse ggplot2 for visualization.\n\n\n\n\nSolution Exercise 5 Bonus: Least polluting region with weights\nlibrary(ggplot2)\n\nregion_weighted &lt;- survey_data %&gt;%\n  group_by(region) %&gt;%\n  summarise(weighted_mean = wtd.mean(co2_per_capita, weights = weight, na.rm = TRUE)) %&gt;%\n  arrange(weighted_mean)\n\n# Least polluting region\nleast_polluting &lt;- region_weighted %&gt;% slice_min(order_by = weighted_mean, n = 1)\nleast_polluting\n\n# Visualization\nggplot(region_weighted, aes(x = reorder(region, weighted_mean), y = weighted_mean, fill = region)) +\n  geom_col(show.legend = FALSE) +\n  coord_flip() +\n  labs(title = \"Weighted Mean CO₂ per Capita by Region\",\n       x = \"Region\", y = \"Weighted CO₂ per Capita\") +\n  theme_minimal()"
  },
  {
    "objectID": "exercises/r-base.html",
    "href": "exercises/r-base.html",
    "title": "Discovering basic  objects",
    "section": "",
    "text": "In this first tutorial, we will gently begin our journey of discovering .\nThis will be done through the following steps:"
  },
  {
    "objectID": "exercises/r-base.html#numeric",
    "href": "exercises/r-base.html#numeric",
    "title": "Discovering basic  objects",
    "section": "2.1 Numeric vectors (numeric)",
    "text": "2.1 Numeric vectors (numeric)\n\n2.1.1 Two types of numeric vectors\n offers different types of numeric objects. For data analysis, we’ll mainly focus on two types:\n\nintegers (type int for integer)\nreal numbers (type double for floating-point numbers)\n\nIn practice, the former are a special case of the latter. Unlike other languages,  doesn’t attempt to automatically constrain whole numbers to be integers. This is convenient but on large data volumes it can be problematic because doubles are heavier than ints.\nGenerally, we use the class function to display the type of a  object and if we want to be more precise we can use typeof:\n\nclass(3)\ntypeof(3)\nclass(3.14)\ntypeof(3.14)\n\nThe as.numeric and as.integer functions can be used to convert from one type to another:\n\n# Conversion to int\nas.integer(3.79)\n\n[1] 3\n\n\n\n\n\n\n\n\nWarning\n\n\n\nBe careful with double \\(\\to\\) int conversion, which truncates the decimal part.\n\n# double -&gt; int -&gt; double\nas.numeric(\n    as.integer(3.79)\n)\n\n[1] 3\n\n\n\n\nDoubles can also be written in scientific notation:\n\n2e3\n\n[1] 2000\n\nclass(2e3)\n\n[1] \"numeric\"\n\n\n\n\n2.1.2 Basic arithmetic operations\n\n\n\n\n\nLike any computer language,  is first and foremost a calculator. We can therefore do additions:\n\n# Addition\n8 + 9\n\n[1] 17\n\n\n\n\n\n\n\n\nNote\n\n\n\n is well designed, it adapts variable types to make them consistent when they can be:\n\n# Addition\n8.1 + as.integer(9)\n\n[1] 17.1\n\n\n\n\nWe of course have access to other standard operations:\n\n# Subtraction\n5 - 2\n\n[1] 3\n\n# Multiplication\n2 * 6\n\n[1] 12\n\n# Division\n9 / 4\n\n[1] 2.25\n\n\nWe still need to be careful with division by 0\n\n# Division by 0\n3 / 0\n\n[1] Inf\n\n-5 / 0\n\n[1] -Inf\n\n\n\n\n\n\n\n\nWarning\n\n\n\nSome languages, like Python, don’t allow division by 0, they return an error rather than Inf. This is a bit tricky in R because we can have divisions by 0 without realizing it…\n\n\nLike any calculator, we can apply other types of operations\n\n# Euclidean division: quotient\n9 %/% 4\n\n[1] 2\n\n# Euclidean division: remainder\n9 %% 4\n\n[1] 1\n\n\n\n# Power\n2 ^ 5\n\n[1] 32\n\n# Square root\nsqrt(5)\n\n[1] 2.236068\n\n# Log\nlog(2)\n\n[1] 0.6931472\n\n# Exponential\nexp(2)\n\n[1] 7.389056\n\n\nThe order of operations follows the usual convention:\n\n2 + 5 * (10 - 4)\n\n[1] 32\n\n\n\n\n2.1.3 Vectorization\nIf we could only use  in basic calculator mode, it wouldn’t be a very interesting language for data analysis. The main advantage of  is that we can manipulate vectors, i.e., sequences of numbers. We’ll consider vectors to be sequences of numbers ordered in a single column:\n\\[\n\\begin{bmatrix} x_1 \\\\ x_2 \\\\ \\vdots \\\\ x_n \\end{bmatrix}\n\\]\nand we’ll apply operations to each row of these vectors. We speak of vectorization of operations to designate an operation that will automatically apply to each element of our vector.\nFor example, multiplication is vectorial by default:\n\n5*c(1,20,2)\n\n[1]   5 100  10\n\n\nSame with addition, as long as we have vectors of consistent size:\n\nc(1,20,2) + c(21,2,20)\n\n[1] 22 22 22\n\nc(1,20,2) - 3\n\n[1] -2 17 -1\n\n\n\n\n\n\n\n\nWarning\n\n\n\nIf the size of vectors isn’t consistent,  recycles the smaller vector until reaching the right size\n\nc(1,20,2) - c(1,20)\n\nWarning in c(1, 20, 2) - c(1, 20): longer object length is not a multiple of\nshorter object length\n\n\n[1] 0 0 1"
  },
  {
    "objectID": "exercises/r-base.html#characters",
    "href": "exercises/r-base.html#characters",
    "title": "Discovering basic  objects",
    "section": "2.2 Character strings (characters)",
    "text": "2.2 Character strings (characters)\nCharacter strings are used to store textual information. More precisely, they can store any Unicode character, which includes letters from different languages, but also punctuation, numbers, smileys, etc.\n\n2.2.1 Creating a string\nTo create a character string, we can use either quotes or apostrophes interchangeably.\n\n1'word'\n2\"this works too\"\n\n\n1\n\nFirst method: '\n\n2\n\nSecond method (preferable): \"\n\n\n\n\n[1] \"word\"\n[1] \"this works too\"\n\n\n\n\n\n\n\n\nWarning\n\n\n\nBe careful about mixing the two!\n\nprint('it's the apostrophe, what a catastrophe')\n\nError in parse(text = input): &lt;text&gt;:1:11: unexpected symbol\n1: print('it's\n              ^\n\n\nThe second apostrophe is understood as the end of the string, and  doesn’t know how to interpret the rest of the sequence.\nWe must therefore vary as needed:\n\n\"it's the apostrophe, no problem\"\n\n[1] \"it's the apostrophe, no problem\"\n\n\n\nThis time, the apostrophe ' is properly nested within the quotes that delimit our string.\n\nThis also works in reverse: quotation marks are properly interpreted when they’re between apostrophes.\n\n'quotation marks, \"no problem\"'\n\n[1] \"quotation marks, \\\"no problem\\\"\"\n\n\nAs the output above shows, it’s possible to properly define special characters of this sort by escaping them with backslashes \\:\n\n1\"quotation marks, \\\"no problem\\\"\"\n'it\\'s the apostrophe, no problem'\n\n\n1\n\n\\ allows  to understand that the apostrophe or quotation mark is part of the character string and not its delimiter.\n\n\n\n\n[1] \"quotation marks, \\\"no problem\\\"\"\n[1] \"it's the apostrophe, no problem\"\n\n\n\n\n\n\n2.2.2 Some useful functions\n provides by default a certain number of useful functions to extract or transform text vectors. We’ll discover more practical and general ones when we focus on textual data and the stringr package.\nThe nchar function counts the number of characters in a string, all characters included (letters, numbers, spaces, punctuation…).\n\nnchar(\"I have 19 characters\")\n\n[1] 20\n\n\nIt shouldn’t be confused with the length function. This one gives us the vector length. For example,\n\nlength(\"I have 19 characters\")\n\n[1] 1\n\n\nis of size 1 since we have a single element in our vector. If we take a larger dimension vector:\n\nlength(c(\"I have 19 characters\", \"not me\"))\n\n[1] 2\n\n\nWe correctly get the number of elements in our vector from length.\nnchar is a vectorial operation, so we can count the length of each row in our dataset:\n\nnchar(c(\"I have 19 characters\", \"not me\"))\n\n[1] 20  6\n\n\nOne of the interests of base text data processing functions is the possibility of automatically reformatting our character strings. For example, the simplest operation is to change the capitalization of our text:\n\n1toupper(c(\"sequence 850\", \"Sequence 850\"))\n2tolower(c(\"SEQuEnce 850\", \"SEQUENCE 850\"))\n\n\n1\n\nPut all text in uppercase.\n\n2\n\nPut in lowercase.\n\n\n\n\n[1] \"SEQUENCE 850\" \"SEQUENCE 850\"\n[1] \"sequence 850\" \"sequence 850\"\n\n\nBut we can also clean text strings with some base functions:\n\nstrsplit(c(\"a sequence    to separate\", \"anothertoseparate\"), split = \" \")\n\n[[1]]\n[1] \"a\"        \"sequence\" \"\"         \"\"         \"\"         \"to\"       \"separate\"\n\n[[2]]\n[1] \"anothertoseparate\"\n\n\nAt this stage, the output obtained, with [[]] may seem strange to you because we haven’t yet discovered the list type.\nSince this type of data isn’t necessarily practical for statistical analysis, for which we prefer formats like vectors, it will be much more practical to use the stringr package to do a split.\nWe can certainly split our string on something other than spaces!\n\nstrsplit(c(\"a sequence    to separate\", \"anothertoseparate\"), split = \"to\")\n\n[[1]]\n[1] \"a sequence    \" \" separate\"     \n\n[[2]]\n[1] \"another\"  \"separate\"\n\n\nWe can concatenate character strings together, it’s very practical. Unfortunately the + doesn’t work in R for character strings (unlike Python). To do this we use paste or paste0 (a less general version but which is designed for simple concatenations):\n\npaste0(\n    \"The first time Aurélien saw Bérénice,\",\n    \" \",\n    \"he found her frankly ugly. She displeased him, in short.\",\n    \" \",\n    \"He didn't like how she was dressed.\"\n1)\n\npaste(\n    \"The first time Aurélien saw Bérénice,\",\n    \"he found her frankly ugly. She displeased him, in short.\",\n    \"He didn't like how she was dressed.\",\n    sep = \" \"\n2)\n\n\n1\n\nWith paste0, we concatenate by joining strings, without spaces.\n\n2\n\nWith paste, we can choose how to join strings, here by adding spaces.\n\n\n\n\n[1] \"The first time Aurélien saw Bérénice, he found her frankly ugly. She displeased him, in short. He didn't like how she was dressed.\"\n[1] \"The first time Aurélien saw Bérénice, he found her frankly ugly. She displeased him, in short. He didn't like how she was dressed.\"\n\n\nWe can use strings as templates. This is particularly practical for automatically creating text from values from our data. For this we use sprintf:\n\nsprintf(\"The first time %s saw %s\", \"Aurélien\", \"Bérénice\")\n\n[1] \"The first time Aurélien saw Bérénice\"\n\nsprintf(\"%s and %s make %s\", 2, 2, 2+2)\n\n[1] \"2 and 2 make 4\"\n\n\n%s is used to define where the desired text will be pasted."
  },
  {
    "objectID": "exercises/r-base.html#logicals",
    "href": "exercises/r-base.html#logicals",
    "title": "Discovering basic  objects",
    "section": "2.3 Logical vectors (logicals)",
    "text": "2.3 Logical vectors (logicals)\nIn , logical vectors are used to store boolean values, i.e., true (TRUE) or false (FALSE) values.\nLogical vectors are commonly used to perform logic operations, data filters and conditional selections. We’ll come back to this later, we’ll use them frequently but indirectly.\n\n15 &gt; 3\n22 == 2\n30 == (2 - 2)\n41 &lt; 0\n\n\n1\n\nTRUE, because 5 is greater than 3.\n\n2\n\nTRUE, because 2 equals 2.\n\n3\n\nTRUE, the operation chain is respected.\n\n4\n\nFALSE, because 1 is not less than 0.\n\n\n\n\n[1] TRUE\n[1] TRUE\n[1] TRUE\n[1] FALSE\n\n\nWe can generalize comparisons to get vectors:\n\nc(2, 4, 6, 8, 10, 1, 3) %% 2 == 0\n\n[1]  TRUE  TRUE  TRUE  TRUE  TRUE FALSE FALSE\n\n\nWe get TRUE for even numbers, FALSE for odd ones.\nUsing logical vectors will allow us, on a daily basis, to select data. For example if we have age data, we may only want to keep adults’ names. This can be done following this model:\n\nc('Pierre', 'Paul', 'François', 'and others')[\n    c(25, 3, 61, 17) &gt;= 18\n]\n\n[1] \"Pierre\"   \"François\"\n\n\nHowever we’ll see in the next chapters how to integrate this principle into a more general sequence of operations thanks to the dplyr package."
  },
  {
    "objectID": "exercises/r-base.html#factors",
    "href": "exercises/r-base.html#factors",
    "title": "Discovering basic  objects",
    "section": "2.4 Factors",
    "text": "2.4 Factors\nFactors are used to represent categorical variables, i.e. variables that take a finite and predetermined number of levels or categories.\nTo convert a numeric or text vector to a factor, we use the factor function:\n\nfactor(\n    c(\"capital\",\"prefecture\",\"sub-prefecture\",\"prefecture\")\n)\n\n[1] capital        prefecture     sub-prefecture prefecture    \nLevels: capital prefecture sub-prefecture\n\nfactor(c(1,10,3))\n\n[1] 1  10 3 \nLevels: 1 3 10\n\n\nThe levels of a factor are the different categories or possible values that the variable can take. We can list them with the levels function\n\nlevels(\n    factor(\n        c(\"capital\",\"prefecture\",\"sub-prefecture\",\"prefecture\")\n    )\n)\n\n[1] \"capital\"        \"prefecture\"     \"sub-prefecture\"\n\n\nWe can also order these levels if it makes sense when defining the factor. This however involves knowing, a priori our different levels and informing  in order:\n\nfactor(\n    c(\"capital\",\"prefecture\",\"sub-prefecture\",\"prefecture\"),\n    levels = c(\"capital\",\"prefecture\",\"sub-prefecture\"),\n    ordered = TRUE\n)\n\n[1] capital        prefecture     sub-prefecture prefecture    \nLevels: capital &lt; prefecture &lt; sub-prefecture"
  },
  {
    "objectID": "exercises/r-base.html#matrices",
    "href": "exercises/r-base.html#matrices",
    "title": "Discovering basic  objects",
    "section": "7.1 Matrices",
    "text": "7.1 Matrices\nMatrices can be seen as the two-dimensional extension of vectors. Instead of having data on a single dimension, we stack columns side by side.\n\\[\nX = \\begin{bmatrix}\nx_{11} & x_{12} \\\\\nx_{21} & x_{22} \\\\\n\\end{bmatrix}\n\\]\nHowever, matrices have a fundamental limitation: we can only store in a matrix elements of the same type. In other words, we’ll exclusively have numeric matrices, character matrices or logical matrices. It’s impossible to build a matrix where some variables are numeric (for example survey respondents’ age) and others are character type (for example their sector of activity).\nMatrices therefore don’t constitute an object type likely to store the statistical information usually used in social surveys. Mixing types isn’t practical, which is why data analysis practitioners use them little3.\nWe therefore propose an exercise on matrices but we’ll quickly move to more flexible types, more useful for data analysis where variables are of diverse types.\n\n\n\n\n\n\nExercise 6\n\n\n\nGiven a matrix:\n\nX &lt;- matrix(letters[1:20], nrow = 4, ncol = 5)\n\n\nSelect the leftmost element of our matrix (first row, first column)\nSelect the entire first row\nSelect the entire first column\nSelect elements at the intersection of:\n\n2nd and 3rd rows\n1st and 3rd columns\n\n\n\n\nSolution\n1X[1,1]\n2X[1,]\n3X[,1]\n4X[2:3,c(1,3)]\n\n\n\n1\n\nSelect the leftmost element of our matrix (first row, first column)\n\n2\n\nSelect the entire first row\n\n3\n\nSelect the entire first column\n\n4\n\nSelect elements at the intersection of:\n\n\n\n\n\n\nHint if you’re stuck\n\nWith a vector, we accessed element positions with X[*]. With matrices the principle is the same but we add a dimension X[*,*]"
  },
  {
    "objectID": "exercises/r-base.html#lists",
    "href": "exercises/r-base.html#lists",
    "title": "Discovering basic  objects",
    "section": "7.2 Lists",
    "text": "7.2 Lists\nLists constitute a much richer object type that precisely allows bringing together very different types of objects: a list can contain all object types (numeric vectors, characters, logicals, matrices, etc.), including other lists.\nThis very great flexibility makes the list the object of choice for storing complex and structured information, particularly results of complex statistical procedures (regression, classification, etc.). For more structured data, as datasets are, we’ll see next that we’ll use a special type of list: the dataframe.\n\n\n\nProposed illustration of the list principle with R by Dall-E-2\n\n\n\n\n\n\n\n\nExercise 7\n\n\n\nHere’s a list illustrating the principle of storing heterogeneous data in the same object:\n\nmy_list &lt;- list(\n    1,\n    \"text\",\n    matrix(letters[1:20], nrow = 4, ncol = 5),\n    c(2, 3, 4)\n)\n\n\nDisplay the list and observe the difference with the display of previous objects\nUse the [[]] notation to access the 2nd element of our list to the 2nd number within the last element of our matrix\nWe can use names for our list’s elements (it’s moreover a good practice). Create an element named municipalities in your list storing the following data c('01363', '02644', '03137', '11311')\nCreate a departments element by extracting the first two digits of your municipalities element\n\n\n\nSolution\n# Question 1: display the list\nmy_list\n# Question 2: Access the second element of the list\nmy_list[[2]]\nmy_list[[4]][2]\n# Question 3: update the list with a named element and access it\nmy_list[['municipalities']] &lt;- c(\n  '01363', '02644', '03137', '11311'\n  )\nmy_list[['municipalities']]  \n# Question 4: perform an operation \nmy_list[['departments']] &lt;- substr(my_list[['municipalities']], start = 1, stop = 2)\n\n\n\n\nWhen using lists, we can perform operations on each element of our list. This is called looping over our list.\n\n\n\n\n\n\nExercise 8\n\n\n\nIn this exercise, we’ll discover how to apply the same function to our list’s elements using lapply.\n\nBefore that, how many elements does the first level of our list have?\nHow many elements does each level of our list have?\nCreate a numeric vector that equals 1 if typeof of the element is “double” and 0 otherwise\n\n\n\nSolution\n# Question 1\nlength(my_list[[1]])\n# Question 2\nlist_length &lt;- length(my_list)\n# Question 3\nas.numeric(\n  lapply(my_list, function(l) typeof(l) == \"double\")\n)\n\n\n\n\nIf ?lapply doesn’t help\n\nExample of using lapply to sum in each element of our list\n\nmy_number_list &lt;- list(c(1,2), seq(1,10))\nlapply(my_number_list, sum)\n\n[[1]]\n[1] 3\n\n[[2]]\n[1] 55"
  },
  {
    "objectID": "exercises/r-base.html#dataframes",
    "href": "exercises/r-base.html#dataframes",
    "title": "Discovering basic  objects",
    "section": "7.3 Dataframes",
    "text": "7.3 Dataframes\nThis is the central object of data analysis with . These objects indeed allow representing in table form (i.e. a two-dimensional object) data of both quantitative nature (numeric variables) and qualitative (character or factor type variables).\n\n\n\nIllustration of the dataframe principle (borrowed from H. Wickham)\n\n\nHere’s for example a dataframe:\n\n# Creating the data.frame df\ndf &lt;- data.frame(\n  var1 = 1:10,\n  var2 = letters[1:10],\n  var3 = rep(c(TRUE, FALSE), times = 5)\n)\n\nIts internal structure can be verified with the str function:\n\nstr(df)\n\n'data.frame':   10 obs. of  3 variables:\n $ var1: int  1 2 3 4 5 6 7 8 9 10\n $ var2: chr  \"a\" \"b\" \"c\" \"d\" ...\n $ var3: logi  TRUE FALSE TRUE FALSE TRUE FALSE ...\n\n\nWhen working with R, one of the functions we use most is head. It displays the first \\(n\\) rows of our dataset:\n\nhead(df)\n\n  var1 var2  var3\n1    1    a  TRUE\n2    2    b FALSE\n3    3    c  TRUE\n4    4    d FALSE\n5    5    e  TRUE\n6    6    f FALSE\n\n\n\n\n\n\n\n\nWarning\n\n\n\nIt’s also possible to use RStudio’s viewer to display datasets.\nBe careful however, this viewer can encounter performance problems and crash your R session when the dataset starts to be of considerable size.\nI recommend rather using head or selecting rows randomly with sample:\n\ndf[sample(nrow(df), 3), ]\n\n  var1 var2  var3\n6    6    f FALSE\n9    9    i  TRUE\n5    5    e  TRUE\n\n\n\n\nFrom a structural point of view, a data.frame is actually a list whose all elements have the same length: this is what allows representing it in the form of a two-dimensional table.\n\nis.list(df)\n\n[1] TRUE\n\nlapply(df, length)\n\n$var1\n[1] 10\n\n$var2\n[1] 10\n\n$var3\n[1] 10\n\n\nTherefore, data.frames borrow their characteristics sometimes from lists, sometimes from matrices as the following exercise shows:\n\n\n\n\n\n\nExercise 9\n\n\n\n\nCheck the dimension of dataframe df\nCount the number of rows and columns of df\nCheck the length (length) of df. Is this the behavior of a matrix or a list?\nExtract the element at the 2nd row, 3rd column of df. Is this the indexing behavior of a matrix or a list?\nRetrieve the 3rd row of variables var1 and var2.\n\n\n\nSolution\ndim(df)\nnrow(df)\nncol(df)\nlength(df) #like a list\ndf[3, c(\"var1\",\"var2\")]\n\n\n\n\nThe interest of using a data.frame is that we can easily update our data during statistical analysis. The most classic operations, which we’ll come back to in the next chapter, are\n\nCreate a new column from pre-existing columns;\nSelect a subsample of data corresponding to certain observed values.\n\nThere are several ways to refer to an already existing column of a dataframe. The simplest is to use the structure dataframe$column. This will give us a vector and we fall back on this format we already know:\n\nclass(df$var1)\n\n[1] \"integer\"\n\n\n\n\n\n\n\n\nExercise 10\n\n\n\n\nCreate a var4 column in our dataset equal to the square of var1\nCreate a var5 column in our dataset concatenating the first two variables generalizing the schema 1=a.\nCreate a df_small1 dataframe for rows where the logical condition var3 is verified\nCreate a df_small2 dataframe for rows where var1 is even (see above the example on Euclidean division for the model)\n\n\n\nThe next chapter will allow us to go much further thanks to the tidyverse ecosystem and particularly its flagship package dplyr. Without this set of packages greatly facilitating statistical analysis,  wouldn’t have become one of the two flagship languages of statistics."
  },
  {
    "objectID": "exercises/r-base.html#footnotes",
    "href": "exercises/r-base.html#footnotes",
    "title": "Discovering basic  objects",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nTo install RStudio yourself, instructions are here. However, for this course, you won’t need to do the installation, we’ll use a preconfigured infrastructure. This way, we’ll have access to the same environment.↩︎\nThis remark may seem trivial but, in computer science, it isn’t. Many languages (Python, C) have indexing that starts at 0, as is the convention in algebra. This means the first element has an index 0, the second index 1 and the last an index \\(n-1\\).↩︎\nThe matrix object will mainly be used by mathematical statistics researchers or algorithm specialists who will manipulate low-level numeric objects.↩︎"
  },
  {
    "objectID": "exercises/ggplot.html",
    "href": "exercises/ggplot.html",
    "title": "Data visualisation with  and ggplot2",
    "section": "",
    "text": "In this practical session, We will learn how to create synthetic graphical representations with , which is very well equipped for this task thanks to the ggplot2 library. This library implements a grammar of graphics that is flexible, consistent, and easy to use.\nIn this course, the practice of visualization will involve replicating charts found on the Paris Open Data page here.\nThis practical session aims to introduce:\nIn this chapter, we will use the following libraries:\nlibrary(scales)\nlibrary(readr)\nlibrary(dplyr)\nlibrary(forcats)\nlibrary(lubridate)\nlibrary(ggplot2)\nlibrary(plotly)\nThen, we will see how to easily create maps in equivalent formats."
  },
  {
    "objectID": "exercises/ggplot.html#data",
    "href": "exercises/ggplot.html#data",
    "title": "Data visualisation with  and ggplot2",
    "section": "1 Data",
    "text": "1 Data\nA subset of Paris Open Data has been made available to facilitate import.\nIt is an extraction, somewhat dated, of the original dataset where only the columns used in this exercise have been retained.\nWe propose downloading these data and saving them to a local file before importing them1. However, we will not do this manually but rather use . Doing this manually would be bad practice in terms of reproducibility.\n\nurl &lt;- \"https://minio.lab.sspcloud.fr/projet-formation/diffusion/python-datascientist/bike.csv\"\n1download.file(url, \"bike.gz\", mode = \"wb\")\n\n\n1\n\nThe .gz extension is important because readr needs it to understand that the file is compressed."
  },
  {
    "objectID": "exercises/ggplot.html#initial-graphical-outputs",
    "href": "exercises/ggplot.html#initial-graphical-outputs",
    "title": "Data visualisation with  and ggplot2",
    "section": "2 Initial graphical outputs",
    "text": "2 Initial graphical outputs\nTrying to produce a perfect visualization on the first attempt is unrealistic. It is more realistic to gradually improve a graphic representation to highlight structural effects in a dataset step by step.\nWe will therefore start by representing the distribution of traffic at the main measurement stations.\nTo do this, we will quickly produce a barplot and then improve it gradually.\nIn this section, we will reproduce the first two charts from the data analysis page: The 10 counters with the highest hourly average and The 10 counters with the highest total counts. The numeric values in the plots will differ from those on the online page, which is normal because we are working with older data.\n\n\n\n\n\n\nExercise 1: Import the data and produce an initial plot\n\n\n\nThe data contains several dimensions suitable for statistical analysis. Therefore, it is necessary first to summarize them with aggregations to create a readable plot.\n\nImport the bike counter data from the bike.gz file;\nKeep the ten counters with the highest average;\n\n\ndf &lt;- readr::read_csv(\"bike.gz\") %&gt;% \n   dplyr::rename(\n    `counter_id` = `Identifiant du compteur`,\n    `Counter name` = `Nom du compteur`,\n    `counting_site_id` = `Identifiant du site de comptage`,\n    `counting_site_name` = `Nom du site de comptage`,\n    `Hourly count` = `Comptage horaire`,\n    `Counting date and time` = `Date et heure de comptage`,\n    `counting_site_installation_date` = `Date d'installation du site de comptage`)\n\n\ndf1 &lt;- df %&gt;%\n  group_by(`Counter name`) %&gt;%\n  summarise(`Hourly count` = mean(`Hourly count`, na.rm = TRUE)) %&gt;%\n  arrange(desc(`Hourly count`)) %&gt;%\n  head(10)\n\n\n\nThe 10 main stations after step 2\n\n\nhead(df1)\n\n# A tibble: 6 × 2\n  `Counter name`                       `Hourly count`\n  &lt;chr&gt;                                         &lt;dbl&gt;\n1 Totem 73 boulevard de Sébastopol S-N           197.\n2 Totem 73 boulevard de Sébastopol N-S           148.\n3 89 boulevard de Magenta NO-SE                  144.\n4 Totem 64 Rue de Rivoli O-E                     140.\n5 102 boulevard de Magenta SE-NO                 137.\n6 72 boulevard Voltaire NO-SE                    124.\n\n\n\nWe can now focus on producing the graphic\n\nFirst, without worrying about style or aesthetics, create the basic structure of the barplot from the data analysis page:\n\n\n\nInitial figure without style\n\n\nggplot(df1, aes(y = `Counter name`, x = `Hourly count`)) +\n  geom_bar(stat = \"identity\")\n\n\n\n\n\nThe next step is to gradually improve this representation to reproduce the version on the Paris Open Data page. The goal is to make it understandable in broad terms before focusing on aesthetics.\n\nFirst, reorder the bars on the y-axis using the reorder function. This makes the figure’s message more readable.\n\n\n\nFigure with reordered bars\n\n\nfigure1 &lt;- ggplot(df1,\n1       aes(y = reorder(`Counter name`, `Hourly count`),\n           x = `Hourly count`)\n       ) +\n  geom_bar(stat = \"identity\")\nfigure1\n\n\n1\n\nReorder Counter Name according to Hourly Count\n\n\n\n\n\n\n\n\n\nModify the aesthetic layer to apply a red color to all bars\n\n\n\nFigurewith red bars\n\n\nfigure1 &lt;- ggplot(df1,\n       aes(y = reorder(`Counter name`, `Hourly count`),\n           x = `Hourly count`)\n       ) +\n  geom_bar(stat = \"identity\", fill = \"red\")\nfigure1\n\n\n\n\n\n\n\nWe now have a representation that starts to convey a message about the data. However, there are still problematic elements (e.g., labels) as well as missing or incorrect elements (axis titles, chart title, etc.).\n\n\n\n\n\n\nExercise 2: Adding some style\n\n\n\nThe figure now conveys a message but is still hard to read.\n\nAt a minimum, label the axes so someone unfamiliar with the data can understand the chart. Use the same axis labels as in the original figure.\n\n\n\nFigure with labeled axes\n\n\nfigure1 &lt;- figure1 + labs(\n  title = \"The 10 counters with the highest hourly average\",\n  x = \"Counter name\",\n  y = \"Hourly average\"\n)\nfigure1\n\n\n1\n\nThe simplest way to label axes in ggplot is with the labs function.\n\n\n\n\n\n\n\n\n\nThe gray background, while characteristic of a ggplot2 figure, is not very polished. Use a more minimal theme to have a white background.\n\n\n\nFigure with white background\n\n\nfigure1 &lt;- figure1 +\n  theme_minimal()\nfigure1\n\n\n\n\n\n\nNext, focus on aesthetic elements. As this is slightly more advanced, we provide the code to update your figure with the following style elements:\n\ntheme(\n  axis.text.x = element_text(angle = 45, hjust = 1, color = \"red\"),\n  axis.title.x = element_text(color = \"red\"),\n  plot.title = element_text(hjust = 0.5),\n  plot.margin = margin(1, 4, 1, 1, \"cm\")\n)\n\n\nFigure after styling\n\n\nfigure1 &lt;- figure1 +\n  theme(\n    axis.text.x = element_text(angle = 45, hjust = 1, color = \"red\"),\n    axis.title.x = element_text(color = \"red\"),\n    plot.title = element_text(hjust = 0.5),\n    plot.margin = margin(1, 4, 1, 1, \"cm\")\n    )\nfigure1\n\n\n\n\n\nFinally, add complexity to the plot by placing numbers on the bars. Using this post, add the mean hourly counts as in the Paris Open Data figure2.\n\n\nfigure1 &lt;- figure1 + \n  geom_text(\n    aes(label=round(`Hourly count`)),\n    position=position_dodge(width=0.9),\n    hjust=-0.5\n    )\n\n\n\n\nThis shows that Boulevard de Sébastopol is the busiest, which will not surprise anyone cycling in Paris. If you are not familiar with Parisian geography, additional visualizations, such as a map, will be needed in a later chapter.\n\nfigure1\n\n\n\n\nThe 10 counters with the highest hourly average\n\n\n\n\n\n\n\n\n\n\nExercise 3: produce a new figure\n\n\n\nRepeat the process for figure 2 (“The 10 counters with the highest total counts”), to create a similar chart.\n\n\ndf2 &lt;- df %&gt;%\n  group_by(`Counter name`) %&gt;%\n  summarise(`Hourly count` = sum(`Hourly count`, na.rm = TRUE)) %&gt;%\n  arrange(desc(`Hourly count`)) %&gt;%\n  head(10)\n\n\n# Create a horizontal bar plot\nfigure2 &lt;- ggplot(df2, aes(y = reorder(`Counter name`, `Hourly count`), x = `Hourly count`)) +\n  geom_bar(stat = \"identity\", fill = \"forestgreen\") +\n  labs(title = \"The 10 counters that recorded the most bicycles\",\n       x = \"Name of the counter\",\n       y = \"The total number of bikes recorded during the selected period\") +\n  theme_minimal() +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1),\n        axis.title.x = element_text(color = \"forestgreen\"),\n        plot.title = element_text(hjust = 0.5),\n        plot.margin = margin(1, 4, 1, 1, \"cm\"))\n\n\nFigure 2 a the end of the exercise\n\n\nfigure2\n\n\n\n\n\n\n\nBarplots are extremely common, but from a semiology perspective, lollipop charts are preferable: they convey the same information with less visual noise (the bar widths in barplots can obscure information).\nHere is an example of the second figure rendered as a lollipop chart:\n\ndf2_lollipop &lt;- df2 %&gt;%\n  mutate(x =  fct_reorder(`Counter name`, `Hourly count` ), y = `Hourly count`)\n\nfigure2_lollipop &lt;- ggplot(df2_lollipop, aes(x=x, y=y)) +\n    geom_segment(aes(xend=x, yend=0), alpha = 0.4) +\n    geom_point(size=5, color=\"forestgreen\") +\n    coord_flip() +\n  labs(title = \"The 10 counters that recorded the most bicycles\",\n       x = \"Name of the counter\",\n       y = \"The total number of bikes recorded during the selected period\")  +\n  theme_minimal() +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1),\n        axis.title.x = element_text(color = \"forestgreen\"),\n        plot.title = element_text(hjust = 0.5),\n        plot.margin = margin(1, 4, 1, 1, \"cm\")) +\n  scale_y_continuous(labels = unit_format(unit = \"M\", scale=1e-6))\n\n\nfigure2_lollipop\n\n\n\n\n\n\nComparison of barplot and lollipop chart\n\n\nfigure2\nfigure2_lollipop\n\n\n\n\n\n\nBarplot\n\n\n\n\n\n\n\nLollipop\n\n\n\n\n\n\nChoose your preferred representation\n\n\n\n\n\n\n\n\n\n\n\nExercise 3 bis (optional): Produce a lollipop chart\n\n\n\nRedo exercise 2, but instead of a barplot, produce a lollipop chart."
  },
  {
    "objectID": "exercises/ggplot.html#first-temporal-aggregation",
    "href": "exercises/ggplot.html#first-temporal-aggregation",
    "title": "Data visualisation with  and ggplot2",
    "section": "3 First temporal aggregation",
    "text": "3 First temporal aggregation\nWe now focus on the spatial dimension of the dataset using two approaches:\n\nA bar chart summarizing the data monthly;\nTime series illustrating temporal dynamics (next section).\n\nTo start, reproduce the third figure, again a barplot. The first step introduces time data and involves a common operation in time series: changing the date format to allow aggregation at a larger time interval.\n\n\n\n\n\n\nExercise 4: monthly count barplot\n\n\n\n\nUse the format function to create a month variable in the YYYY-MM format ;\nCompute the mean hourly counts for each month\n\n\ndf &lt;- df %&gt;%\n  mutate(month = format(`Counting date and time`, \"%Y-%m\"))\n\n\n# Question 2\nmonthly_hourly_count &lt;- df %&gt;%\n  group_by(month) %&gt;%\n  summarise(value = mean(`Hourly count`, na.rm = TRUE))\n\n\n\nHourly counts obtained after this step\n\n\nmonthly_hourly_count\n\n# A tibble: 14 × 2\n   month   value\n   &lt;chr&gt;   &lt;dbl&gt;\n 1 2019-08  33.6\n 2 2019-09  55.8\n 3 2019-10  49.9\n 4 2019-11  36.0\n 5 2019-12  67.9\n 6 2020-01  66.1\n 7 2020-02  43.2\n 8 2020-03  29.4\n 9 2020-04  12.5\n10 2020-05  54.6\n11 2020-06  85.0\n12 2020-07  80.7\n13 2020-08  53.2\n14 2020-09  98.3\n\n\n\nApply previous recommendations to gradually construct and improve a chart similar to the third figure on the Paris Open Data page.\n\nfigure3 &lt;- ggplot(monthly_hourly_count) +\n  geom_bar(aes(x = month, y = value), fill = \"#ffcd00\", stat = \"identity\") +\n  labs(x = \"Date and time of count\", y = \"Average hourly count per month\\nover the selected period\",\n       title = \"Average monthly bicycle counts\") +\n  theme_minimal() +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1),\n        axis.title.y = element_text(color = \"#ffcd00\", face = \"bold\"),\n        plot.title = element_text(hjust = 0.5),\n        plot.margin = margin(1, 4, 1, 1, \"cm\"))\n\n\n\nExample figure reproducing Paris Open Data\n\n\nfigure3\n\n\n\n\n\n\nOptional: represent the same information as lollipop chart\n\n\n\n\nfigure3\n\n\n\n\nIf you rather present this with a lollipop3 chart:\n\nggplot(monthly_hourly_count, aes(x = month, y = value)) +\n  geom_segment(aes(xend = month, yend = 0)) +\n  geom_point(color=\"#ffcd00\", size=4) +\n  labs(x = \"Counting date and time\", y = \"Average hourly count per month\\nover the selected period\",\n       title = \"Average monthly bicycle counts\") +\n  theme_minimal() +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1),\n        #axis.title.y = element_text(color = \"#ffcd00\", face = \"bold\"),\n        plot.title = element_text(hjust = 0.5),\n        plot.margin = margin(1, 4, 1, 1, \"cm\"))"
  },
  {
    "objectID": "exercises/ggplot.html#first-time-series",
    "href": "exercises/ggplot.html#first-time-series",
    "title": "Data visualisation with  and ggplot2",
    "section": "4 First time series",
    "text": "4 First time series\nTime series visualizations are more common for data with a temporal dimension.\n\n\n\n\n\n\nExercise 5: Time series representation\n\n\n\n\nCreate a day variable converting timestamps to daily format (e.g. 2021-05-01).\n\n\ndf &lt;- df %&gt;%\n  mutate(day = date(`Counting date and time`))\n\ndaily_average &lt;- df %&gt;%\n  group_by(day) %&gt;%\n  summarise(value = mean(`Hourly count`, na.rm = TRUE))\n\n\nPlot this as a time series without worrying about styling;\n\n\nfigure4 &lt;- ggplot(daily_average, aes(x = day, y = value)) +\n  geom_line(color = \"magenta\")\n\n\n\nFigure minimaliste\n\n\nfigure4\n\n\n\n\n\n\nFill the area under the line using the appropriate function;\n\n\nfigure4 &lt;- figure4 +\n  geom_area(fill=\"magenta\", alpha = 0.6)\n\n\n\nFigure avec la coloration sous la ligne\n\n\nfigure4\n\n\n\n\n\n\nFinalize the plot to reproduce the Paris Open Data figure;\n\n\nfigure4 &lt;- figure4 +\n  labs(x = \"Counting date and time (Day)\", y = \"Average hourly count per day\\nover the selected period\",\n       title = \"Average daily bicycle counts\") +\n  theme_minimal() +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1),\n        plot.title = element_text(hjust = 0.5),\n        plot.margin = margin(1, 4, 1, 1, \"cm\"))\n\n\n\nFigure with filled area\n\n\nfigure4\n\n\n\n\n\nSome hints for this exercise\n\n\n💡 Hint for question 1\n\nLook up the day function of the lubridate package\n\n\n\n💡 Hint for question 1\n\nThis stackoverflow thread might help you out.\n\n\n\nData for Figure 4\n\n\nhead(daily_average)\n\n# A tibble: 6 × 2\n  day        value\n  &lt;date&gt;     &lt;dbl&gt;\n1 2019-08-01  46.7\n2 2019-08-02  40.0\n3 2019-08-03  30.7\n4 2019-08-04  28.9\n5 2019-08-05  36.0\n6 2019-08-06  29.9\n\n\n\n\n\nAfter this exercise, the figure will look like this:\n\nfigure4"
  },
  {
    "objectID": "exercises/ggplot.html#introduction-to-html-plots-with-plotly",
    "href": "exercises/ggplot.html#introduction-to-html-plots-with-plotly",
    "title": "Data visualisation with  and ggplot2",
    "section": "5 Introduction to HTML plots with Plotly",
    "text": "5 Introduction to HTML plots with Plotly\nThe drawback of ggplot figures is that they are not interactive. All information must be included in the plot, which can make it harder to read. If well done, a multi-level information plot can still work.\nWith web technologies, it is easier to offer multi-level visualizations: a first glance conveys the main message, and the user can explore additional details interactively. Interactive visualizations, now standard in dataviz, allow the reader to hover for more information or click to reveal additional details.\nThese visualizations rely on the web triptych: HTML, CSS, and JavaScript.  users do not manipulate these languages directly, but use R libraries that automatically generate the necessary code.\n\n\n\n\n\n\nExercise 6: interactive time series\n\n\n\n\nCreate a basic Plotly figure representing figure 4 as a time series without worrying about style.\n\n\n\nTime series figure without styling\n\n\nplot_ly(\n  daily_average, x = ~day, y = ~value,\n  type = 'scatter', mode = 'lines'\n)\n\n\n\n\n\n\n\nUsing the documentation, add an area under the curve;\n\n\n\nAdd area under the line\n\n\nplot_ly(\n  daily_average, x = ~day, y = ~value,\n  fill = 'tozeroy',\n  type = 'scatter', mode = 'lines'\n)\n\n\n\n\n\n\n\nAdjust styling elements to reproduce figure 4. Use hovertemplate and hoverinfo to enhance interactivity.\n\n\nfig &lt;- plot_ly(\n  daily_average, x = ~day, y = ~value,\n  color = I(\"magenta\"),\n  hovertemplate = ~paste(day, \": \", round(value), \" bicycle passages per hour on average\"),\n  hoverinfo = \"text\",\n  fill = 'tozeroy',\n  type = 'scatter', mode = 'lines')\nfig4 &lt;- fig %&gt;%\n  layout(title = \"Average daily bicycle counts\",\n         xaxis = list(title = \"Counting date and time (Day)\"),\n         yaxis = list(title = \"Average hourly count per day\\nover the selected period\"))\n\n\n\nFigure obtenue\n\n\nfig4\n\n\n\n\n\n\n\n\nThe interactive version is then produced.\n\nfig4\n\n\n\n\n\nThis shows the special characteristics of 2020. To highlight the first lockdown visible in the data, vertical lines can be added using documentation.\n\nvline &lt;- function(x = 0, color = \"royalblue\") {\n  list(\n    type = \"line\",\n    y0 = 0,\n    y1 = 1,\n    yref = \"paper\",\n    x0 = x,\n    x1 = x,\n    line = list(color = color, dash=\"dot\")\n  )\n}\n\nfig4 %&gt;% layout(shapes = list(vline(\"2020-03-17\"), vline(\"2020-05-11\")))\n\n\n\n\n\nFinally, here’s how to reproduce this figure with Plotly:\n\ndf1 &lt;- df1 %&gt;% mutate(`Counter name` = fct_reorder(`Counter name`, `Hourly count`))\n\nfig &lt;- plot_ly(\n  df1,\n  x = ~ `Hourly count`, y = ~`Counter name`,\n  color = I(\"red\"),\n  hovertext = ~paste0(`Counter name`, \": \", round(`Hourly count`)),\n  hoverinfo = 'text',\n  type = 'bar',\n  name = 'Main stations')\n\n\nfig &lt;- fig %&gt;% layout(\n  yaxis = list(title = 'Hourly average'),\n  xaxis = list(title = 'Counter name', color = \"red\")\n  )\n\n\nfig\n\n\n\n\n\n\n\n\n\n\n\nExercise 7: barplot with Plotly\n\n\n\n\nTo order bars correctly, use fct_reorder from forcats on the dataframe from exercise 1;\nUse Plotly to create your figure;\n(Optional, advanced) Produce a lollipop chart with Plotly"
  },
  {
    "objectID": "exercises/ggplot.html#footnotes",
    "href": "exercises/ggplot.html#footnotes",
    "title": "Data visualisation with  and ggplot2",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nNormally, we recommend using the download URL directly to avoid creating an intermediate file on the disk. However, direct import with readr will not work here because the library does not recognize that the file is compressed without the .gz extension.↩︎\nNote: displaying numbers on bars is not necessarily best practice in dataviz. It can make the scale and data variation less immediately understandable.↩︎\nI removed the color on the y-axis, which I find adds little to the figure and even detracts from the message’s clarity.↩︎"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Introduction to quantitative methods with ",
    "section": "",
    "text": "Introduction to quantitative methods with \nThis course is based on the course of Lino Galiana for the Department of Social Sciences at ENS Ulm.\nA course to become familiar with  through hands-on practice using open data.\n\n\n\n  \n     \n      \n          \n            \n          \n          \n            Importing and manipulating data with \n            \n              \nIntroduction to \nand to RStudio\n\n\n              \n                Clara Baudry, Nathan Randriamanana\n                Feb 16, 2026\n              \n            \n            \n                \n                  Exercise ✏️\n                \n                \n                  Slides 🧑‍🏫\n                \n            \n          \n        \n      \n     \n  \n  \n     \n      \n          \n            \n          \n          \n            Manipulate data with the tidyverse\n            \n              \nIntroduction to the tidyverse\nto import and manipulate data with \n\n\n              \n                Clara Baudry, Nathan Randriamanana\n                Feb 17, 2026\n              \n            \n            \n                \n                  Exercise ✏️\n                \n                \n                  Slides 🧑‍🏫\n                \n            \n          \n        \n      \n     \n  \n  \n     \n      \n          \n            \n          \n          \n            Statistical analysis\n            \n              \nBasic and intermediate statistical analysis with \n\n\n              \n                Clara Baudry, Nathan Randriamanana\n                Feb 18, 2026\n              \n            \n            \n                \n                  Exercise ✏️\n                \n                \n                  Slides 🧑‍🏫\n                \n            \n          \n        \n      \n     \n  \n  \n     \n      \n          \n            \n          \n          \n            Create data visualizations with ggplot2\n            \n              \nIntroduction to ggplot2,\nto create plots with\n\n\n\n              \n                Clara Baudry, Nathan Randriamanana\n                Feb 19, 2026\n              \n            \n            \n                \n                  Exercise ✏️\n                \n                \n                  Slides 🧑‍🏫\n                \n            \n          \n        \n      \n     \n  \n  \n     \n      \n          \n            \n          \n          \n            Reproducible report generation with quarto\n            \n              \nIntroduction to quarto,\nto generate reports with\n\n\n\n              \n                Clara Baudry, Nathan Randriamanana\n                Feb 20, 2026\n              \n            \n            \n                \n                  Exercise ✏️\n                \n                \n                  Slides 🧑‍🏫\n                \n            \n          \n        \n      \n     \n  \n\n\nNo matching items"
  },
  {
    "objectID": "slides/ggplot.html#introduction",
    "href": "slides/ggplot.html#introduction",
    "title": "Introduction to Quantitative Methods with ",
    "section": "Introduction",
    "text": "Introduction\n\n\n\n\n\n\nNote\n\n\n\nExercises associated with this chapter here\n\n\n\n\n\n\nBack to the main page"
  },
  {
    "objectID": "slides/ggplot.html#introduction-1",
    "href": "slides/ggplot.html#introduction-1",
    "title": "Introduction to Quantitative Methods with ",
    "section": "Introduction",
    "text": "Introduction\n\nData visualization synthesizes data structure\n\nGuides further data exploration\nConveys a message to the reader\n\n\n\n\n2 types of visualizations:\n\nStatic representations: ggplot2\nHTML representations: leaflet"
  },
  {
    "objectID": "slides/ggplot.html#example-dataset",
    "href": "slides/ggplot.html#example-dataset",
    "title": "Introduction to Quantitative Methods with ",
    "section": "Example dataset",
    "text": "Example dataset\n\nlibrary(doremifasol)\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(scales)\n\ndf &lt;- telechargerDonnees(\"FILOSOFI_DISP_IRIS\", date = 2017) %&gt;%\n  as_tibble() %&gt;%\n  sample_frac(0.1)\n\n\n\n\n\n\n\nNote\n\n\nInspiration for these slides:\n\nCollaborative documentation utilitR;\nggplot manual;\nCourse by Joseph Larmarange"
  },
  {
    "objectID": "slides/ggplot.html#ggplot2",
    "href": "slides/ggplot.html#ggplot2",
    "title": "Introduction to Quantitative Methods with ",
    "section": "ggplot2",
    "text": "ggplot2\nInitialize a figure associated with a dataset\n\nggplot(df, aes(x = DISP_MED17, y = DISP_D917))"
  },
  {
    "objectID": "slides/ggplot.html#ggplot2-1",
    "href": "slides/ggplot.html#ggplot2-1",
    "title": "Introduction to Quantitative Methods with ",
    "section": "ggplot2",
    "text": "ggplot2\nAdd layers (+) with geom_* functions\n\nggplot(df) +\n  geom_point(aes(x = DISP_MED17, y = DISP_D917))"
  },
  {
    "objectID": "slides/ggplot.html#ggplot2-2",
    "href": "slides/ggplot.html#ggplot2-2",
    "title": "Introduction to Quantitative Methods with ",
    "section": "ggplot2",
    "text": "ggplot2\nParameterize layers with aes\n\nggplot(df) +\n  geom_point(aes(x = DISP_MED17, y = DISP_D917, color = DISP_Q117), shape = 3)\n\n\n\n\n\n\n\n\nNote\n\n\nAesthetic control of a geom_ layer is done through:\n\naes: variable parameters of the layer linked to a variable;\noutside aes: parameters that apply uniformly to the layer"
  },
  {
    "objectID": "slides/ggplot.html#ggplot2-3",
    "href": "slides/ggplot.html#ggplot2-3",
    "title": "Introduction to Quantitative Methods with ",
    "section": "ggplot2",
    "text": "ggplot2\nAdd layers (+) with geom_* functions\n\nggplot(df, aes(x = DISP_MED17, y = DISP_D917)) +\n  geom_point(aes(color = DISP_Q117), shape = 3) +\n  geom_smooth(color = \"red\", alpha = 0.7, se = FALSE)"
  },
  {
    "objectID": "slides/ggplot.html#ggplot2-4",
    "href": "slides/ggplot.html#ggplot2-4",
    "title": "Introduction to Quantitative Methods with ",
    "section": "ggplot2",
    "text": "ggplot2\nModify scales with scale_ functions\n\nggplot(df, aes(x = DISP_MED17, y = DISP_D917)) +\n  geom_point(aes(color = DISP_Q117), shape = 3) +\n  geom_smooth(color = \"red\", alpha = 0.7, se = FALSE) +\n  scale_x_continuous(labels = unit_format(unit = \"k\", scale=1e-3)) +\n  scale_y_continuous(trans='log', labels = unit_format(unit = \"k\", scale=1e-3)) +\n  scale_color_viridis_c()"
  },
  {
    "objectID": "slides/ggplot.html#ggplot2-5",
    "href": "slides/ggplot.html#ggplot2-5",
    "title": "Introduction to Quantitative Methods with ",
    "section": "ggplot2",
    "text": "ggplot2\nModify scales with scale_ functions\n\ndf &lt;- df %&gt;% mutate(quartile = factor(ntile(DISP_Q117, 4)))\nggplot(df, aes(x = DISP_MED17, y = DISP_D917)) +\n  geom_point(aes(color = quartile), shape = 3) +\n  geom_smooth(color = \"red\", alpha = 0.7, se = FALSE) +\n  scale_x_continuous(labels = unit_format(unit = \"k\", scale=1e-3)) +\n  scale_y_continuous(trans='log', labels = unit_format(unit = \"k\", scale=1e-3)) +\n  scale_color_viridis_d(option = \"turbo\")"
  },
  {
    "objectID": "slides/ggplot.html#ggplot2-6",
    "href": "slides/ggplot.html#ggplot2-6",
    "title": "Introduction to Quantitative Methods with ",
    "section": "ggplot2",
    "text": "ggplot2\nModify aesthetics, only at the end\n\np &lt;- ggplot(df, aes(x = DISP_MED17, y = DISP_D917)) +\n  geom_point(aes(color = quartile), shape = 3) +\n  geom_smooth(color = \"red\", alpha = 0.7, se = FALSE) +\n  scale_x_continuous(labels = unit_format(unit = \"k\", scale=1e-3)) +\n  scale_y_continuous(trans='log', labels = unit_format(unit = \"k\", scale=1e-3)) +\n  scale_color_viridis_d(option = \"turbo\")\n\np + theme_bw() +\n  labs(x = \"Median income\", y = \"9th decile\", color = \"Quartile\") +\n  theme(legend.position = \"bottom\")\n\n\n\n\nIntroduction to Quantitative Methods with , École Normale Supérieure (back to main page)"
  },
  {
    "objectID": "slides/wrangling.html#introductory-chapter",
    "href": "slides/wrangling.html#introductory-chapter",
    "title": "Introduction to Quantitative Methods with ",
    "section": "Introductory Chapter",
    "text": "Introductory Chapter\n\n\n\n\n\n\nNote\n\n\n\nExercises associated with this chapter here\n\n\n\n\n\n\nBack to the main page"
  },
  {
    "objectID": "slides/wrangling.html#introduction",
    "href": "slides/wrangling.html#introduction",
    "title": "Introduction to Quantitative Methods with ",
    "section": "Introduction",
    "text": "Introduction\n\n centered around the dataframe\n\n\n\nBut manipulations sometimes a bit cumbersome:\n\ndf$var: a bit heavy\ndf[,]: a bit too modeled on matrices\n\n\n\n\n\nTextual data: a perfectible base\n\nConvoluted outputs\nMissing functionalities compared to Python"
  },
  {
    "objectID": "slides/wrangling.html#the-answer-the-tidyverse",
    "href": "slides/wrangling.html#the-answer-the-tidyverse",
    "title": "Introduction to Quantitative Methods with ",
    "section": "The answer: the tidyverse!",
    "text": "The answer: the tidyverse!\n\nThe tidyverse ecosystem"
  },
  {
    "objectID": "slides/wrangling.html#the-answer-the-tidyverse-1",
    "href": "slides/wrangling.html#the-answer-the-tidyverse-1",
    "title": "Introduction to Quantitative Methods with ",
    "section": "The answer: the tidyverse!",
    "text": "The answer: the tidyverse!\nA set of packages developed by RStudio that facilitates:\n\nReading (readr) and manipulating databases (dplyr)\nExploiting textual data (stringr), temporal (lubridate) or categorical (forcats)\nCreating graphics (ggplot2)\nProgramming from dataframes (purrr)\nAnd many other things…"
  },
  {
    "objectID": "slides/wrangling.html#the-tidy-data-concept",
    "href": "slides/wrangling.html#the-tidy-data-concept",
    "title": "Introduction to Quantitative Methods with ",
    "section": "The tidy data concept",
    "text": "The tidy data concept\n\nEach variable has its own column;\nEach observation has its own row;\nA value, materializing an observation of a variable, is found in a single cell.\n\n\n\n\n\n\n\nNote\n\n\nConcept popularized by Hadley Wickham."
  },
  {
    "objectID": "slides/wrangling.html#readr",
    "href": "slides/wrangling.html#readr",
    "title": "Introduction to Quantitative Methods with ",
    "section": "readr",
    "text": "readr\n\n\nThe package for reading flat files (.csv, .txt…)\nAllows obtaining a tibble, the augmented dataframe of the tidyverse"
  },
  {
    "objectID": "slides/wrangling.html#dplyr",
    "href": "slides/wrangling.html#dplyr",
    "title": "Introduction to Quantitative Methods with ",
    "section": "dplyr",
    "text": "dplyr\n\n\nThe central package of the data manipulation ecosystem;\nData manipulation and descriptive statistics;"
  },
  {
    "objectID": "slides/wrangling.html#dplyr-1",
    "href": "slides/wrangling.html#dplyr-1",
    "title": "Introduction to Quantitative Methods with ",
    "section": "dplyr",
    "text": "dplyr\nMain verbs\nWe work on a tibble (augmented dataframe)\n\nselect(): select variables by their name;\nrename(): rename variables;\nfilter(): select observations according to one or more conditions;\narrange(): sort the table according to one or more variables;\nmutate(): add variables that are functions of other variables;\nsummarise(): calculate a statistic from data;\ngroup_by(): perform operations by group."
  },
  {
    "objectID": "slides/wrangling.html#dplyr-2",
    "href": "slides/wrangling.html#dplyr-2",
    "title": "Introduction to Quantitative Methods with ",
    "section": "dplyr",
    "text": "dplyr\nData manipulation\n\n\nThe following package(s) will be installed:\n- eurostat [4.0.0]\nThese packages will be installed into \"/__w/r-introduction/r-introduction/renv/library/linux-ubuntu-noble/R-4.5/x86_64-pc-linux-gnu\".\n\n# Installing packages --------------------------------------------------------\n- Installing eurostat 4.0.0 ...                 OK [linked from cache]\nSuccessfully installed 1 package in 5.8 milliseconds.\n\n\nWe chain sequences with pipes (%&gt;%)\n\nlibrary(dplyr)\n\npop_by_age %&gt;%\n1  as_tibble() %&gt;%\n2  filter(sex == \"F\") %&gt;%\n3  mutate(x = paste0(age, \"_\", sex))\n\n\n1\n\nWe convert the standard dataframe to tibble\n\n2\n\nWe keep only gas stations (values B316 of TYPEQU)\n\n3\n\nWe create a new column by referencing existing ones (without quotes!)\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\nMore details in utilitR"
  },
  {
    "objectID": "slides/wrangling.html#dplyr-3",
    "href": "slides/wrangling.html#dplyr-3",
    "title": "Introduction to Quantitative Methods with ",
    "section": "dplyr",
    "text": "dplyr\nAggregated statistics\n\nsplit-apply-combine logic with groupby\n\n\n\nIllustration of split-apply-combine\n\n\n\n\nlibrary(dplyr)\n\npop_by_age %&gt;%\n1  as_tibble() %&gt;%\n2  filter(TIME_PERIOD == \"2025-01-01\") %&gt;%\n3  group_by(geo) %&gt;%\n4  summarise(pop_by_country_2025 = sum(values, na.rm = TRUE))\n\n\n1\n\nWe convert the standard dataframe to tibble\n\n2\n\nWe keep only gas stations (values 2025-01-01 of TIME_PERIOD)\n\n3\n\nWe define geo as a stratification variable to define groups\n\n4\n\nWe summarize the data by summing gas stations in each group"
  },
  {
    "objectID": "slides/wrangling.html#ggplot",
    "href": "slides/wrangling.html#ggplot",
    "title": "Introduction to Quantitative Methods with ",
    "section": "ggplot",
    "text": "ggplot\n\n\nThe essential package for making graphics ❤️;\nCoherent and flexible approach based on the grammar of graphics\n\nSubject of a dedicated chapter"
  },
  {
    "objectID": "slides/wrangling.html#stringr-forcats-and-lubridate",
    "href": "slides/wrangling.html#stringr-forcats-and-lubridate",
    "title": "Introduction to Quantitative Methods with ",
    "section": "stringr, forcats and lubridate",
    "text": "stringr, forcats and lubridate\n\n  \n\n\nMany functions facilitating manipulation:\n\nTextual data: stringr\nCategorical data: forcats\nTemporal data: lubridate"
  },
  {
    "objectID": "slides/wrangling.html#general-information",
    "href": "slides/wrangling.html#general-information",
    "title": "Introduction to Quantitative Methods with ",
    "section": "General Information",
    "text": "General Information\n\nData can be stored in many different formats\n\nDifferent standards\nDifferent ways of importing\n\n\n\n\nLimited  functionalities:\n\nSpecialized packages for certain formats\nObjective: flatten information into a dataframe\n\n\n\n\n\n\n\n\nNote\n\n\nWe’ll see geographic formats, and their challenges, later"
  },
  {
    "objectID": "slides/wrangling.html#csv",
    "href": "slides/wrangling.html#csv",
    "title": "Introduction to Quantitative Methods with ",
    "section": "CSV",
    "text": "CSV\n\nFlat file format with delimited columns:\n\nStandard: , as delimiter, . as decimal;\nEuropean variant 😮‍💨: ; as delimiter, , as decimal\n\nUniversal format, simple to use (some limitations)\n\n\nviewof info_csv = Inputs.radio(\n  [\"Raw file\", \"File after import\"], {value: \"Raw file\"}\n)\n\n\n\n\n\n\n\ninfo_csv == \"Raw file\" ? html`&lt;div&gt;${md_csv}&lt;div&gt;` : html`&lt;div&gt;${df_csv}&lt;div&gt;`\n\n\n\n\n\n\n\ndf_csv = Inputs.table(\n  d3.csvParse(raw_csv)\n)\nmd_csv = md`\n\\`\\`\\`\n${raw_csv}\n\\`\\`\\`\n`\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nraw_csv = `DEP,REG,CHEFLIEU,TNCC,NCC,NCCENR,LIBELLE\n01,84,01053,5,AIN,Ain,Ain\n02,32,02408,5,AISNE,Aisne,Aisne\n03,84,03190,5,ALLIER,Allier,Allier\n04,93,04070,4,ALPES DE HAUTE PROVENCE,Alpes-de-Haute-Provence,Alpes-de-Haute-Provence\n05,93,05061,4,HAUTES ALPES,Hautes-Alpes,Hautes-Alpes\n06,93,06088,4,ALPES MARITIMES,Alpes-Maritimes,Alpes-Maritimes\n`"
  },
  {
    "objectID": "slides/wrangling.html#csv-1",
    "href": "slides/wrangling.html#csv-1",
    "title": "Introduction to Quantitative Methods with ",
    "section": "CSV",
    "text": "CSV\n\nReading with the read_csv function from the readr package!\n\n\n1library(readr)\n2read_csv(\"data_folder/file_name.csv\")\n\n\n1\n\nWe import the readr library to have access to the read_csv function\n\n2\n\nWe use read_csv to read data stored in the relative path data_folder/file_name.csv\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nCSV with ; delimiter: read_csv2 function.\nMore exotic flat formats (.txt for example): read_delim\n\nMore details in the utilitR documentation"
  },
  {
    "objectID": "slides/wrangling.html#json",
    "href": "slides/wrangling.html#json",
    "title": "Introduction to Quantitative Methods with ",
    "section": "JSON",
    "text": "JSON\n\nThe web format, especially for APIs\n\nAPI: we’ll see that later\n\n\n\nviewof info_json = Inputs.radio(\n  [\"Raw file\", \"File after import\"], {value: \"Raw file\"}\n)\n\n\n\n\n\n\n\ninfo_json == \"Raw file\" ? html`&lt;div&gt;${md_json}&lt;div&gt;` : html`&lt;div&gt;${df_json}&lt;div&gt;`\n\n\n\n\n\n\n\njson_string = `[\n  {\"DEP\": \"01\", \"REG\": \"84\", \"CHEFLIEU\": \"01053\", \"TNCC\": \"5\", \"NCC\": \"AIN\", \"NCCENR\": \"Ain\", \"LIBELLE\": \"Ain\"},\n  {\"DEP\": \"02\", \"REG\": \"32\", \"CHEFLIEU\": \"02408\", \"TNCC\": \"5\", \"NCC\": \"AISNE\", \"NCCENR\": \"Aisne\", \"LIBELLE\": \"Aisne\"},\n  {\"DEP\": \"03\", \"REG\": \"84\", \"CHEFLIEU\": \"03190\", \"TNCC\": \"5\", \"NCC\": \"ALLIER\", \"NCCENR\": \"Allier\", \"LIBELLE\": \"Allier\"},\n  {\"DEP\": \"04\", \"REG\": \"93\", \"CHEFLIEU\": \"04070\", \"TNCC\": \"4\", \"NCC\": \"ALPES DE HAUTE PROVENCE\", \"NCCENR\": \"Alpes-de-Haute-Provence\", \"LIBELLE\": \"Alpes-de-Haute-Provence\"},\n  {\"DEP\": \"05\", \"REG\": \"93\", \"CHEFLIEU\": \"05061\", \"TNCC\": \"4\", \"NCC\": \"HAUTES ALPES\", \"NCCENR\": \"Hautes-Alpes\", \"LIBELLE\": \"Hautes-Alpes\"},\n  {\"DEP\": \"06\", \"REG\": \"93\", \"CHEFLIEU\": \"06088\", \"TNCC\": \"4\", \"NCC\": \"ALPES MARITIMES\", \"NCCENR\": \"Alpes-Maritimes\", \"LIBELLE\": \"Alpes-Maritimes\"}\n]`\nraw_json = [\n  {\"DEP\": \"01\", \"REG\": \"84\", \"CHEFLIEU\": \"01053\", \"TNCC\": \"5\", \"NCC\": \"AIN\", \"NCCENR\": \"Ain\", \"LIBELLE\": \"Ain\"},\n  {\"DEP\": \"02\", \"REG\": \"32\", \"CHEFLIEU\": \"02408\", \"TNCC\": \"5\", \"NCC\": \"AISNE\", \"NCCENR\": \"Aisne\", \"LIBELLE\": \"Aisne\"},\n  {\"DEP\": \"03\", \"REG\": \"84\", \"CHEFLIEU\": \"03190\", \"TNCC\": \"5\", \"NCC\": \"ALLIER\", \"NCCENR\": \"Allier\", \"LIBELLE\": \"Allier\"},\n  {\"DEP\": \"04\", \"REG\": \"93\", \"CHEFLIEU\": \"04070\", \"TNCC\": \"4\", \"NCC\": \"ALPES DE HAUTE PROVENCE\", \"NCCENR\": \"Alpes-de-Haute-Provence\", \"LIBELLE\": \"Alpes-de-Haute-Provence\"},\n  {\"DEP\": \"05\", \"REG\": \"93\", \"CHEFLIEU\": \"05061\", \"TNCC\": \"4\", \"NCC\": \"HAUTES ALPES\", \"NCCENR\": \"Hautes-Alpes\", \"LIBELLE\": \"Hautes-Alpes\"},\n  {\"DEP\": \"06\", \"REG\": \"93\", \"CHEFLIEU\": \"06088\", \"TNCC\": \"4\", \"NCC\": \"ALPES MARITIMES\", \"NCCENR\": \"Alpes-Maritimes\", \"LIBELLE\": \"Alpes-Maritimes\"}\n]\nmd_json = md`\n\\`\\`\\`\n${json_string}\n\\`\\`\\`\n`\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ndf_json = Inputs.table(raw_json)"
  },
  {
    "objectID": "slides/wrangling.html#json-1",
    "href": "slides/wrangling.html#json-1",
    "title": "Introduction to Quantitative Methods with ",
    "section": "JSON",
    "text": "JSON\n\nImported as a hierarchical list\nObjective: transform this information into a tidy dataframe\n\nNot always easy!\n\n\n\n1library(jsonlite)\n2df &lt;- fromJSON(file=\"data_folder/file_name.json\")\n\n\n1\n\nWe import the jsonlite library to have access to the fromJSON function\n\n2\n\nWe use fromJSON to read data stored in the relative path data_folder/file_name.json"
  },
  {
    "objectID": "slides/wrangling.html#excel-formats",
    "href": "slides/wrangling.html#excel-formats",
    "title": "Introduction to Quantitative Methods with ",
    "section": "Excel formats",
    "text": "Excel formats\n\nProprietary format\nMixes formatting and raw data\n\nNot appropriate for data analysis\nDangerous for reproducibility and transparency\n\nMore details on utilitR"
  },
  {
    "objectID": "slides/wrangling.html#the-magrittr-pipe-1",
    "href": "slides/wrangling.html#the-magrittr-pipe-1",
    "title": "Introduction to Quantitative Methods with ",
    "section": "The magrittr pipe (%>%)",
    "text": "The magrittr pipe (%&gt;%)\nA way to chain operations\n\n\nNested functionsWith the pipe\n\n\n\n\n# A tibble: 49 × 2\n   geo   pop_by_country_2025\n   &lt;chr&gt;               &lt;dbl&gt;\n 1 AD                1938528\n 2 AL               66085816\n 3 AM               41834200\n 4 AT              523911792\n 5 AZ              180389320\n 6 BE              677625197\n 7 BG              529118397\n 8 BY              123719805\n 9 CH              462691936\n10 CY               26631335\n# ℹ 39 more rows\n\n\n\n\n\n\n# A tibble: 40 × 2\n   geo   pop_by_country_2025\n   &lt;chr&gt;               &lt;dbl&gt;\n 1 AL                2362991\n 2 AT                9195525\n 3 AZ               10223305\n 4 BE               11880328\n 5 BG                6436921\n 6 CH                9048831\n 7 CY                 982819\n 8 CZ               10908523\n 9 DE               83559239\n10 DK                5991469\n# ℹ 30 more rows\n\n\n\n\n\n\n\nIntroduction to Quantitative Methods with , École Normale Supérieure (back to main page)"
  },
  {
    "objectID": "slides/introduction.html#introductory-chapter",
    "href": "slides/introduction.html#introductory-chapter",
    "title": "Introduction to quantitative methods with ",
    "section": "Introductory Chapter",
    "text": "Introductory Chapter\n\n\n\n\n\n\nNote\n\n\n\nExercises associated with this chapter here\n\n\n\n\n\n\nBack to main page"
  },
  {
    "objectID": "slides/introduction.html#scan-what-you-need",
    "href": "slides/introduction.html#scan-what-you-need",
    "title": "Introduction to quantitative methods with ",
    "section": "Scan what you need",
    "text": "Scan what you need\n\n\nIntroduction slides\n\n\n    \n\nWebsite"
  },
  {
    "objectID": "slides/introduction.html#about-us",
    "href": "slides/introduction.html#about-us",
    "title": "Introduction to quantitative methods with ",
    "section": "About us",
    "text": "About us\n\n2 french instructors 🇫🇷\ncolleagues from INSEE: National Institute of Statistics and Economic Studies"
  },
  {
    "objectID": "slides/introduction.html#all-about-us",
    "href": "slides/introduction.html#all-about-us",
    "title": "Introduction to quantitative methods with ",
    "section": "All about us",
    "text": "All about us\nInstructors\n\nNathan Randriamanana"
  },
  {
    "objectID": "slides/introduction.html#all-about-us-1",
    "href": "slides/introduction.html#all-about-us-1",
    "title": "Introduction to quantitative methods with ",
    "section": "All about us",
    "text": "All about us\nInstructors\n\nNew character unlocking tomorrow! 🔒 🤩 🥳"
  },
  {
    "objectID": "slides/introduction.html#who-am-i-then",
    "href": "slides/introduction.html#who-am-i-then",
    "title": "Introduction to quantitative methods with ",
    "section": "Who am I then ?",
    "text": "Who am I then ?\n\nData scientist at Insee\n\nINSEE statistician civil servant\nworking now at the Business Statistics Directorate\nSIRENE business register"
  },
  {
    "objectID": "slides/introduction.html#about-my-job",
    "href": "slides/introduction.html#about-my-job",
    "title": "Introduction to quantitative methods with ",
    "section": "About my job",
    "text": "About my job\n\nI wear two hats:\n\n🎩 Application Administrator: Acting as the functional lead for the SIRENE register. I bridge the gap between business needs and IT by writing functional specifications for system maintenance and evolution.\n🧢 Data Scientist: Managing the end-to-end ML workflow for APE classification from model training and delivery to its integration into the production application.\n\n\n\nExample from presentation at the Cloud Native Days France 2026 conference regarding cloud technologies"
  },
  {
    "objectID": "slides/introduction.html#socials",
    "href": "slides/introduction.html#socials",
    "title": "Introduction to quantitative methods with ",
    "section": "Socials",
    "text": "Socials\nI may not post every day, but I’d love to stay in touch!\n\n\nLinkedIn\n\n\n    \n\nGitHub"
  },
  {
    "objectID": "slides/introduction.html#about-my-courses",
    "href": "slides/introduction.html#about-my-courses",
    "title": "Introduction to quantitative methods with ",
    "section": "About my courses",
    "text": "About my courses\nYou might also be interested in my other introductory courses:\n\nGitHub organization for all my courses; this is just the beginning ! 🚀\n\nMost importantly, check out the full training portal on the Insee datalab"
  },
  {
    "objectID": "slides/introduction.html#getting-to-know-each-other",
    "href": "slides/introduction.html#getting-to-know-each-other",
    "title": "Introduction to quantitative methods with ",
    "section": "Getting to know each other",
    "text": "Getting to know each other\n\nLet’s introduce ourselves\nWhat are you looking for in this course?\n\nAny topics you are passionate about?\nAny specific expectations?"
  },
  {
    "objectID": "slides/introduction.html#training-roadmap",
    "href": "slides/introduction.html#training-roadmap",
    "title": "Introduction to quantitative methods with ",
    "section": "Training roadmap",
    "text": "Training roadmap\n\nDay 1: Setting the stage with hands-on R and fundamental data manipulation\nDay 2: Data cleaning and transformation 🧹 🪄\nDay 3: Descriptive and inferential statistics 🔎🎲\nDay 4: Data visualisation (ggplot2) 📊\nDay 5: Reporting with Quarto and final project 👑"
  },
  {
    "objectID": "slides/introduction.html#why-are-we-doing-this",
    "href": "slides/introduction.html#why-are-we-doing-this",
    "title": "Introduction to quantitative methods with ",
    "section": "Why are we doing this?",
    "text": "Why are we doing this?\n\nEducational goals: Upskilling and fostering a culture of technical autonomy.\nStrategic stakes: Ensuring institutional independence, cost-efficiency, and scientific transparency."
  },
  {
    "objectID": "slides/introduction.html#educational-goals",
    "href": "slides/introduction.html#educational-goals",
    "title": "Introduction to quantitative methods with ",
    "section": "1. Educational goals",
    "text": "1. Educational goals\n\nGaining technical depth: Moving from “black-box” tools to understanding the underlying code and logic.\nDeveloping a self-service culture: Building the autonomy to create, debug, and improve your own solutions."
  },
  {
    "objectID": "slides/introduction.html#educational-goals-1",
    "href": "slides/introduction.html#educational-goals-1",
    "title": "Introduction to quantitative methods with ",
    "section": "1. Educational goals",
    "text": "1. Educational goals\n\nThe open data & open source ecosystems:\n\nopen data: Learning to leverage public datasets as a primary resource.\nopen source: Discovering community-driven tools (R, Quarto) to process them.\n\nHands-on introduction to : Learning the fundamentals of R through practical application.\nIntroduction to reproducible publications with Quarto: Learn to create automated, transparent, and high-quality reports."
  },
  {
    "objectID": "slides/introduction.html#strategic-stakes",
    "href": "slides/introduction.html#strategic-stakes",
    "title": "Introduction to quantitative methods with ",
    "section": "2. Strategic Stakes",
    "text": "2. Strategic Stakes\n\nPublic fund stewardship & financial independence:\n\nOptimizing national budget: Redirecting taxpayer money from recurring proprietary licenses toward internal expertise and innovation.\nCost-efficiency: Scaling tools without proportional increases in software costs.\n\nInstitutional Sovereignty:\n\nFull ownership: Controlling our own statistical production chain without being tied to a vendor’s “black-box” roadmap."
  },
  {
    "objectID": "slides/introduction.html#strategic-stakes-1",
    "href": "slides/introduction.html#strategic-stakes-1",
    "title": "Introduction to quantitative methods with ",
    "section": "2. Strategic Stakes",
    "text": "2. Strategic Stakes\n\nInstitutional Sovereignty:\n\nSustainability: Ensuring long-term access to our code and methods, independent of private sector pricing policies.\n\nScientific & Public Trust:\n\nReproducibility: Guaranteeing that official statistics are auditable and transparent (open science standards).\nModernization: Maintaining the Institute’s position at the state-of-the-art of data science."
  },
  {
    "objectID": "slides/introduction.html#what-you-stand-to-gain",
    "href": "slides/introduction.html#what-you-stand-to-gain",
    "title": "Introduction to quantitative methods with ",
    "section": "What you stand to gain",
    "text": "What you stand to gain\n\nUnderstand the strategic value of open source in today’s landscape\nLearn professional best practices 💎\nGain independence from proprietary solutions 🏢\nConnect and collaborate with the global data community 🌍\nDemystify the “industrialization” of data through practice\n\n➡️ Build higher technical expertise"
  },
  {
    "objectID": "slides/introduction.html#practical-information",
    "href": "slides/introduction.html#practical-information",
    "title": "Introduction to quantitative methods with ",
    "section": "Practical Information",
    "text": "Practical Information\n\nA mix of slides and, most importantly, guided practice sessions.\nComputing infrastructure (SSPCloud) provided by Insee to avoid:\n\nInstallation headaches.\nConfiguration struggles.\n\nSetup instructions to follow shortly.\n\nWe will be using SSPCloud 😍🐉☁️🇫🇷!\n(Quick walkthrough coming up later)"
  },
  {
    "objectID": "slides/introduction.html#additional-resources",
    "href": "slides/introduction.html#additional-resources",
    "title": "Introduction to quantitative methods with ",
    "section": "Additional resources",
    "text": "Additional resources\n\nComprehensive english references\n\nR for data science – a deep dive into the tidyverse ecosystem.\nGeocomputation with R – a complete guide for spatial data and mapping.\nData visualization – a practical and thorough introduction by Kieran Healy.\n\nInteractive learning platforms\n\nSSPCloud training portal – provides interactive, self-paced tracks on modern data stacks (available in english)."
  },
  {
    "objectID": "slides/introduction.html#additional-resources-1",
    "href": "slides/introduction.html#additional-resources-1",
    "title": "Introduction to quantitative methods with ",
    "section": "Additional resources",
    "text": "Additional resources\n\nCommunity and french resources (worth a look)\n\nutilitR – the most comprehensive r documentation 👶. (tip: great to explore using browser auto-translate).\nRzine – best practices and tutorials for social sciences and geography."
  },
  {
    "objectID": "slides/introduction.html#data-proliferation",
    "href": "slides/introduction.html#data-proliferation",
    "title": "Introduction to quantitative methods with ",
    "section": "Data proliferation",
    "text": "Data proliferation\n\nDigitalization and technological innovation have slashed the cost of data production.\n\nExponential growth in the volume of data generated.\n\nThe use of statistics for governance is not new (cf. Desrosières or Ian Hacking)…\n… but numbers now hold a central place in public debate and policy-making (Supiot, Davies)."
  },
  {
    "objectID": "slides/introduction.html#data-diversification-14",
    "href": "slides/introduction.html#data-diversification-14",
    "title": "Introduction to quantitative methods with ",
    "section": "Data diversification (1/4)",
    "text": "Data diversification (1/4)\nClassic tabular data\n\nStructured data in table format (rows and columns). \n is exceptionally well-equipped for this (handled via dataframes)."
  },
  {
    "objectID": "slides/introduction.html#data-diversification-24",
    "href": "slides/introduction.html#data-diversification-24",
    "title": "Introduction to quantitative methods with ",
    "section": "Data diversification (2/4)",
    "text": "Data diversification (2/4)\nGeospatial data\n\nTabular data with a spatial dimension.\n\nGeography comes in multiple forms: points, lines, polygons…\n\n offers powerful tools for this type of data (as long as data volume fits in memory)."
  },
  {
    "objectID": "slides/introduction.html#data-diversification-34",
    "href": "slides/introduction.html#data-diversification-34",
    "title": "Introduction to quantitative methods with ",
    "section": "Data diversification (3/4)",
    "text": "Data diversification (3/4)\nTextual and unstructured data\n\nHistorical statistical roots (Levenshtein 1957, perceptron).\nRapid development since 2010:\n\nMassive collection: social media, open-ended survey questions…\nLower storage costs and increased computing power.\nNew techniques: webscraping, Natural Language Processing (NLP), and LLMs.\n\nHeavy usage across government, research, and the private sector."
  },
  {
    "objectID": "slides/introduction.html#data-diversification-44",
    "href": "slides/introduction.html#data-diversification-44",
    "title": "Introduction to quantitative methods with ",
    "section": "Data diversification (4/4)",
    "text": "Data diversification (4/4)\nImages, sound and video\n\nComputer vision and signal processing are now part of the statistical toolkit (e.g., analyzing satellite imagery for agricultural yields)."
  },
  {
    "objectID": "slides/introduction.html#emergence-of-new-players",
    "href": "slides/introduction.html#emergence-of-new-players",
    "title": "Introduction to quantitative methods with ",
    "section": "Emergence of new players",
    "text": "Emergence of new players\n\nTraditional actors:\n\nNational Statistical Institutes (Insee, BoS Lesotho) and line ministries;\nCentral administrations (Tax authorities, Digital agencies) or mapping agencies (e.g., IGN);\nMore details to follow.\n\nCrowdsourced and collaborative projects:\n\nOpenStreetMap Lesotho (essential for local infrastructure mapping);\nWikidata, OpenFoodFacts…"
  },
  {
    "objectID": "slides/introduction.html#emergence-of-new-players-1",
    "href": "slides/introduction.html#emergence-of-new-players-1",
    "title": "Introduction to quantitative methods with ",
    "section": "Emergence of new players",
    "text": "Emergence of new players\n\nPrivate sectors:\n\nVast datasets collected from users and customers (Big Data);\nNew opportunities for data-sharing partnerships (e.g., for research or public health);\nThe challenge: How to integrate these non-traditional sources into official statistics? (ex: UNECE)"
  },
  {
    "objectID": "slides/introduction.html#the-democratization-of-data",
    "href": "slides/introduction.html#the-democratization-of-data",
    "title": "Introduction to quantitative methods with ",
    "section": "The democratization of data",
    "text": "The democratization of data\n\nThe rise of Open Data and Open Source:\n\nGlobal momentum for government transparency (starting in the late 2000s).\nRapid growth of national and international open data portals (World Bank, UN, National Platforms).\n\nTechnological and cultural shifts:\n\nGeneralization of open and standardized formats.\nMass adoption of open-source programming languages (especially Python  and R ).\nIncreasing use of APIs for direct data retrieval.\n\n\n\n\n\n\n\n\nSee European Union website\n\n\n“Open (Government) Data refers to the information collected, produced or paid for by the public bodies (also referred to as Public Sector Information) and made freely available for re-use for any purpose. The licence will specify the terms of use. These principles for Open Data are described in detail in the Open Definition.”"
  },
  {
    "objectID": "slides/introduction.html#data-is-everywhere",
    "href": "slides/introduction.html#data-is-everywhere",
    "title": "Introduction to quantitative methods with ",
    "section": "Data is everywhere",
    "text": "Data is everywhere"
  },
  {
    "objectID": "slides/introduction.html#national-statistical-institutes-bos-insee",
    "href": "slides/introduction.html#national-statistical-institutes-bos-insee",
    "title": "Introduction to quantitative methods with ",
    "section": "National Statistical Institutes (BoS & Insee)",
    "text": "National Statistical Institutes (BoS & Insee)\n\nCore missions: Both the Bureau of Statistics (Lesotho) and Insee (France) share the same fundamental goals:\n\nCollecting and analyzing vital data: Census, GDP, Inflation (CPI), and Labor Force Surveys.\n\nSpecific feature: Insee also has a strong mandate for social and economic research to inform public debate.\nThe R Ecosystem:\n\nMany NSI datasets are now directly accessible through R packages.\nThis ensures reproducibility: your analysis can be updated instantly when the BoS or Insee releases new data."
  },
  {
    "objectID": "slides/introduction.html#national-open-data-portals",
    "href": "slides/introduction.html#national-open-data-portals",
    "title": "Introduction to quantitative methods with ",
    "section": "National Open Data Portals",
    "text": "National Open Data Portals\n\nThe Hub: A single platform centralizing data from all public sectors (Health, Transport, Local districts).\nTransparency: Allows citizens and researchers to find and reuse raw data.\nThe R advantage: Instead of manual downloads, we use R to query these portals directly, ensuring our analysis stays up-to-date."
  },
  {
    "objectID": "slides/introduction.html#why-does-this-matter-for-this-training",
    "href": "slides/introduction.html#why-does-this-matter-for-this-training",
    "title": "Introduction to quantitative methods with ",
    "section": "Why does this matter for this training ?",
    "text": "Why does this matter for this training ?\n\nIn the past, we had to “ask” for data. Today, we programmatically access it. Whether you are at BoS Lesotho, Insee, or a mapping agency, R is the bridge between these portals and your analysis."
  },
  {
    "objectID": "slides/introduction.html#github-fa-brands-github-where-the-code-lives",
    "href": "slides/introduction.html#github-fa-brands-github-where-the-code-lives",
    "title": "Introduction to quantitative methods with ",
    "section": "GitHub : Where the code lives",
    "text": "GitHub : Where the code lives\n\nCollaborative platform: A global hub for sharing and hosting code.\nMuch more than just code:\n\nProject documentation and tutorials;\nProfessional websites and dashboards (like the ones we can build with R).\n\nThe home of Open Source and Reproducible Research:\n\nWhere NSIs (like Insee, Stats NZ, or UK ONS) share their methodology;\nAllows for transparent and verifiable statistical production."
  },
  {
    "objectID": "slides/introduction.html#the-principle-of-an-open-source-language",
    "href": "slides/introduction.html#the-principle-of-an-open-source-language",
    "title": "Introduction to quantitative methods with ",
    "section": "The principle of an open source language",
    "text": "The principle of an open source language\n\n\n\n\n\n\nGeneral principle\n\n\n\n\n\n\n\nIllustration with R"
  },
  {
    "objectID": "slides/introduction.html#what-is-r",
    "href": "slides/introduction.html#what-is-r",
    "title": "Introduction to quantitative methods with ",
    "section": "What is R?",
    "text": "What is R?\n\nAn open source statistical software:\n\nCore language for base operations\nPackages to extend functionalities\n\nWidely adopted across academia and public administrations\nExtensive online support and resources\n\n\n\n\n\n\n\nNote\n\n\n\nCreated in the 1990s;\nMassive growth since 2010 (rising alongside Python).\nRStudio: the next-generation IDE that makes R accessible and powerful for data science."
  },
  {
    "objectID": "slides/introduction.html#a-swiss-army-knife-software",
    "href": "slides/introduction.html#a-swiss-army-knife-software",
    "title": "Introduction to quantitative methods with ",
    "section": "A “swiss army knife” software",
    "text": "A “swiss army knife” software\n\nHandling all types of data;\nData visualization (dataviz), mapping and GIS;\nModeling (machine learning, network analysis…)\nWriting reports, websites, and slides (like these ones 🤓)…"
  },
  {
    "objectID": "slides/introduction.html#a-swiss-army-knife-software-1",
    "href": "slides/introduction.html#a-swiss-army-knife-software-1",
    "title": "Introduction to quantitative methods with ",
    "section": "A “swiss army knife” software",
    "text": "A “swiss army knife” software\nYou can do everything in R:\n\nExcerpt from R for data science (the bible)"
  },
  {
    "objectID": "slides/introduction.html#transparency-and-reproducibility",
    "href": "slides/introduction.html#transparency-and-reproducibility",
    "title": "Introduction to quantitative methods with ",
    "section": "Transparency and reproducibility",
    "text": "Transparency and reproducibility\n\nTraceability of statistics and graphical outputs.\nSharing R code ensures methodological transparency:\n\nMore and more journals now require code submissions!\nStill some progress to be made in the field.\n\nUsing R Markdown (or Quarto) increases efficiency 🐢🔜🐇:\n\nEliminates messy intermediate files (text, excel, images…).\nSaves time on formatting (millions of hours saved, literally).\n\n\n\n\n\n\n\n\nNote\n\n\nSee the dedicated course on best practices (Insee is highly involved in this topic)."
  },
  {
    "objectID": "slides/introduction.html#a-community-of-users",
    "href": "slides/introduction.html#a-community-of-users",
    "title": "Introduction to quantitative methods with ",
    "section": "A community of users",
    "text": "A community of users\n\nAn open source software:\n\nFree and collaborative.\n\nThousands of packages:\n\non CRAN (The Comprehensive R Archive Network).\non GitHub.\n\nA community driven by open science ideals.\nA bridge between disciplines: sociology, economics, biology, political science, etc.\nThe power of collaboration: R thrives because users build their own tools and share them with the world."
  },
  {
    "objectID": "slides/introduction.html#the-rstudio-interface",
    "href": "slides/introduction.html#the-rstudio-interface",
    "title": "Introduction to quantitative methods with ",
    "section": "The RStudio Interface",
    "text": "The RStudio Interface\n\nThe four main panes of RStudio"
  },
  {
    "objectID": "slides/introduction.html#the-rstudio-interface-1",
    "href": "slides/introduction.html#the-rstudio-interface-1",
    "title": "Introduction to quantitative methods with ",
    "section": "The RStudio Interface",
    "text": "The RStudio Interface\n\nThe four main panes of RStudio\nSource (Top Left): Your script editor. This is where you write and save your code.\nConsole (Bottom Left): Where the code actually runs. You can type commands here for quick tests.\nEnvironment (Top Right): Shows your active data, variables, and history.\nOutput Panes (Bottom Right): Where you see your plots, files, and help pages."
  },
  {
    "objectID": "slides/introduction.html#is-rstudio-really-the-next-generation-ide",
    "href": "slides/introduction.html#is-rstudio-really-the-next-generation-ide",
    "title": "Introduction to quantitative methods with ",
    "section": "Is RStudio really the next-generation IDE ?",
    "text": "Is RStudio really the next-generation IDE ?\n\nRStudio is now Posit: The company changed its name to show they support more than just R (Python, Julia, etc.).\nThe rise of VS Code: Many data scientists are moving toward VS Code, a universal editor that is becoming the industry standard.\nWhy change ?: VS Code is faster, supports multiple languages better, and has a massive ecosystem of extensions.\n\n\n\n\n\n\n\nThe take-away\n\n\nLearn RStudio today because it’s the easiest for beginners. But keep an eye on VS Code, it is likely where the future of professional data science is heading."
  },
  {
    "objectID": "slides/introduction.html#why-move-from-rstudio-to-vs-code",
    "href": "slides/introduction.html#why-move-from-rstudio-to-vs-code",
    "title": "Introduction to quantitative methods with ",
    "section": "Why move from RStudio to VS Code ?",
    "text": "Why move from RStudio to VS Code ?\n\nPolyglot: Work on R , Python , and SQL  in the same window.\nPerformance: More lightweight and stable for very large projects.\nStandardization: It’s the same tool used by software engineers worldwide.\nPositron: Posit is even building a new IDE based on VS Code technology!"
  },
  {
    "objectID": "slides/introduction.html#what-is-the-ssp-cloud",
    "href": "slides/introduction.html#what-is-the-ssp-cloud",
    "title": "Introduction to quantitative methods with ",
    "section": "What is the SSP Cloud?",
    "text": "What is the SSP Cloud?"
  },
  {
    "objectID": "slides/introduction.html#what-is-the-ssp-cloud-1",
    "href": "slides/introduction.html#what-is-the-ssp-cloud-1",
    "title": "Introduction to quantitative methods with ",
    "section": "What is the SSP Cloud?",
    "text": "What is the SSP Cloud?\n\nA cloud-native playground for data science (powered by Onyxia)\nHigh-performance servers with R and RStudio pre-installed\nAn open space to learn, experiment, and share code\n\n\n\n\n\n\n\nImportant: Data Security\n\n\nThis platform is for open data only. Do not upload any confidential or sensitive production data from the BoS. Keep your official datasets on your secured local servers.\n\n\n\n\n\n\n\n\n\nNote\n\n\nMore details available in the SSP Cloud documentation"
  },
  {
    "objectID": "slides/introduction.html#why-use-the-ssp-cloud",
    "href": "slides/introduction.html#why-use-the-ssp-cloud",
    "title": "Introduction to quantitative methods with ",
    "section": "Why use the SSP Cloud ?",
    "text": "Why use the SSP Cloud ?\n\nNo installation headache: no need to manage R, RStudio, or packages locally\nStandardized environment: everyone uses the same versions for reproducible work\nOne-click launch: start a full workspace in seconds (using “Training” buttons)"
  },
  {
    "objectID": "slides/introduction.html#creating-your-account",
    "href": "slides/introduction.html#creating-your-account",
    "title": "Introduction to quantitative methods with ",
    "section": "Creating your account",
    "text": "Creating your account\n\nUse your official @gov.ls address at datalab.sspcloud.fr\nUsername rules: No accents, no special characters, no punctuation.\n\n\nUse a simple format like firstname.lastname. For example, if you are Ntate Stunna, your username could be ntatestunna."
  },
  {
    "objectID": "slides/introduction.html#launching-an-rstudio-service",
    "href": "slides/introduction.html#launching-an-rstudio-service",
    "title": "Introduction to quantitative methods with ",
    "section": "Launching an RStudio service",
    "text": "Launching an RStudio service\nQuick Guide\n\nClick on Service Catalog on the left menu"
  },
  {
    "objectID": "slides/introduction.html#launching-an-rstudio-service-1",
    "href": "slides/introduction.html#launching-an-rstudio-service-1",
    "title": "Introduction to quantitative methods with ",
    "section": "Launching an RStudio service",
    "text": "Launching an RStudio service\nQuick Guide\n\nKeep the default RStudio settings and launch"
  },
  {
    "objectID": "slides/introduction.html#launching-an-rstudio-service-2",
    "href": "slides/introduction.html#launching-an-rstudio-service-2",
    "title": "Introduction to quantitative methods with ",
    "section": "Launching an RStudio service",
    "text": "Launching an RStudio service\nQuick Guide\n\nRetrieve your RStudio service password"
  },
  {
    "objectID": "slides/introduction.html#launching-an-rstudio-service-3",
    "href": "slides/introduction.html#launching-an-rstudio-service-3",
    "title": "Introduction to quantitative methods with ",
    "section": "Launching an RStudio service",
    "text": "Launching an RStudio service\nQuick Guide\n\nAlternative way to find your service password"
  },
  {
    "objectID": "slides/introduction.html#launching-an-rstudio-service-4",
    "href": "slides/introduction.html#launching-an-rstudio-service-4",
    "title": "Introduction to quantitative methods with ",
    "section": "Launching an RStudio service",
    "text": "Launching an RStudio service\nQuick Guide\n\nLog in to your service"
  },
  {
    "objectID": "slides/introduction.html#we-want-you-to-lead-the-innovation",
    "href": "slides/introduction.html#we-want-you-to-lead-the-innovation",
    "title": "Introduction to quantitative methods with ",
    "section": "We want YOU to lead the innovation",
    "text": "We want YOU to lead the innovation\n\n\nCreate your space: Don’t just code alone. Start a Slack channel, a Teams group, or an internal forum to help each other.\nOnyxia is a helper, not a cage: It is a facilitator based on open standards. Your code remains yours and stays portable.\nContinuous Exchange: For those inspired to lead or teach others, we will offer one day of remote follow-up to exchange ideas, troubleshoot together, and help you kickstart your internal community.\n\n\n\n\n\n\n\nTake the lead\n\n\nThe best way to learn is to teach others. Build your BoS community today and we will help you become the next generation of instructors."
  },
  {
    "objectID": "slides/stats.html#introductory-chapter",
    "href": "slides/stats.html#introductory-chapter",
    "title": "Introduction to quantitative methods with ",
    "section": "Introductory Chapter",
    "text": "Introductory Chapter\n\n\n\n\n\n\nNote\n\n\n\nExercises associated with this chapter here\n\n\n\n\n\n\nBack to the main page"
  },
  {
    "objectID": "slides/stats.html#course-outline",
    "href": "slides/stats.html#course-outline",
    "title": "Introduction to quantitative methods with ",
    "section": "Course outline",
    "text": "Course outline\n\n1️⃣ Descriptive statistics and data analysis strategies\n2️⃣ Inferential statistics through survey use cases"
  },
  {
    "objectID": "slides/stats.html#the-logic-of-data-aggregation-1",
    "href": "slides/stats.html#the-logic-of-data-aggregation-1",
    "title": "Introduction to quantitative methods with ",
    "section": "The logic of data aggregation",
    "text": "The logic of data aggregation\nThe operations we practiced are part of a standard industry pattern called Split-Apply-Combine. It is the most robust way to solve complex data problems by decomposing them.\n\nSplit: Break a large dataset into manageable, independent pieces (e.g., by strata, region, or gender).\nApply: Operate on each piece independently (calculate a mean, fit a model, or generate a sequence).\nCombine: Reassemble the results into a structured output (like a summary table)."
  },
  {
    "objectID": "slides/stats.html#from-concept-to-code",
    "href": "slides/stats.html#from-concept-to-code",
    "title": "Introduction to quantitative methods with ",
    "section": "From Concept to Code",
    "text": "From Concept to Code\nIn the tidyverse, this strategy is mapped to specific, highly optimized functions:\n\ngroup_by(): The Split operator. It partitions the dataset into logical subsets.\nsummarise(): The Apply & Combine operator. It reduces many rows into a single row of statistical indicators.\nacross(): The Scaling operator. It ensures that the exact same statistical treatment is applied consistently across multiple columns."
  },
  {
    "objectID": "slides/stats.html#practical-aggregation",
    "href": "slides/stats.html#practical-aggregation",
    "title": "Introduction to quantitative methods with ",
    "section": "Practical Aggregation",
    "text": "Practical Aggregation\nlibrary(dplyr)\n\n# Standard descriptive pipeline\ndata %&gt;%\n  group_by(category) %&gt;%\n  summarise(\n    n = n(),\n    across(c(income, age), list(avg = ~mean(.x, na.rm = TRUE), med = ~median(.x, na.rm = TRUE)))\n  )\n\n\n\n\n\n\nWhy this matters for statisticians ?\n\n\nStandardization is the bedrock of valid statistical reporting.\n\nacross() ensures that the same treatment (handling of NAs, rounding, or specific estimators) is applied identically to every variable.\nThis workflow creates a clear link between raw data and final indicators, which is essential for auditability."
  },
  {
    "objectID": "slides/stats.html#a-zoom-on-summarise",
    "href": "slides/stats.html#a-zoom-on-summarise",
    "title": "Introduction to quantitative methods with ",
    "section": "A zoom on summarise()",
    "text": "A zoom on summarise()\nThe summarise() function is the primary tool for data reduction. It transforms a detailed table into a concise summary of statistical indicators.\n\nAggregation Logic: It collapses multiple rows into a single result per group.\nVersatility: You can calculate several statistics at once (Mean, Median, Sum, etc.).\nFunctional Chain: It is almost always paired with group_by() to provide comparative insights across categories."
  },
  {
    "objectID": "slides/stats.html#common-statistical-functions",
    "href": "slides/stats.html#common-statistical-functions",
    "title": "Introduction to quantitative methods with ",
    "section": "Common Statistical Functions",
    "text": "Common Statistical Functions\n\n\n\nCategory\nR Functions\n\n\n\n\nCentral Tendency\nmean(), median()\n\n\nDispersion\nsd(), var(), IQR(), min(), max()\n\n\nPosition\nquantile(x, probs = 0.75), first(), last()\n\n\nCount\nn() (observations), n_distinct() (unique values)"
  },
  {
    "objectID": "slides/stats.html#statistical-estimation",
    "href": "slides/stats.html#statistical-estimation",
    "title": "Introduction to quantitative methods with ",
    "section": "Statistical estimation",
    "text": "Statistical estimation\nThe power of summarise() lies in its ability to combine multiple estimators within a single reduction step. You can use standard R functions, library-specific tools, or even user-defined functions.\nsummarize() core estimators reference\n\n\n\nStatistic\nR Function\nUse Case\n\n\n\n\nMean\nmean()\nCentral tendency (sensitive to outliers)\n\n\nMedian\nmedian()\nRobust central tendency\n\n\nStandard Deviation\nsd()\nDispersion/Spread\n\n\nExtrema\nmin(), max()\nRange of the distribution\n\n\nPosition\nfirst(), last()\nBoundary observations\n\n\nCount\nn()\nSample size (\\(n\\))\n\n\nUniqueness\nn_distinct()\nCardinality check\n\n\nSummation\nsum(), cumsum()\nTotal and cumulative flows"
  },
  {
    "objectID": "slides/stats.html#summarize-example-of-practical-implementation",
    "href": "slides/stats.html#summarize-example-of-practical-implementation",
    "title": "Introduction to quantitative methods with ",
    "section": "summarize() example of practical implementation",
    "text": "summarize() example of practical implementation\nlibrary(dplyr)\n\n# Summary of the entire table\ndata %&gt;%\n  summarise(\n    avg_income = mean(income, na.rm = TRUE),\n    total_obs = n()\n  )\n\n# Strategic aggregation by group\ndata %&gt;%\n  group_by(education_level) %&gt;%\n  summarise(\n    n = n(),\n    med_age = median(age, na.rm = TRUE),\n    max_seniority = max(seniority, na.rm = TRUE)\n  )\n\n\n\n\n\n\nTip: Missing Values\n\n\nIn R, most statistical functions return NA if a single missing value is present. Always include na.rm = TRUE to ensure your summary is computed on the available data."
  },
  {
    "objectID": "slides/stats.html#mutate-vs-summarise",
    "href": "slides/stats.html#mutate-vs-summarise",
    "title": "Introduction to quantitative methods with ",
    "section": "mutate() vs summarise()",
    "text": "mutate() vs summarise()\nBoth functions create new variables, but they operate on different “dimensions” of your data.\nThe key difference\n\n\n\n\n\n\n\n\nFunction\nAction\nResulting row count\n\n\n\n\nmutate()\nTransformation\nRemains unchanged (\\(n = N\\)).\n\n\nsummarise()\nAggregation\nReduced to one row per group (\\(n = G\\))."
  },
  {
    "objectID": "slides/stats.html#mutate-vs-summarise-1",
    "href": "slides/stats.html#mutate-vs-summarise-1",
    "title": "Introduction to quantitative methods with ",
    "section": "mutate() vs summarise()",
    "text": "mutate() vs summarise()\nWhich one should I use?\nA simple rule of thumb:\n\nUse mutate() if you want to keep the individual granularity.\n\nExample: Calculating the tax amount for each household or the log of income. Each person stays in the table.\n\nUse summarise() if you want to move to a higher level of analysis.\n\nExample: Calculating the average income per region. Individual households “disappear” to form the regional indicator.\n\n\nThe hybrid case: grouped mutate\nStatisticians frequently use group_by() %&gt;% mutate() to perform standardization or relative positioning without losing the individual granularity.\n\nsummarise(): You calculate the mean of the group. The individuals are gone.\nmutate(): You calculate the mean of the group and broadcast it back to every individual in that group."
  },
  {
    "objectID": "slides/stats.html#the-specificity-of-survey-data-1",
    "href": "slides/stats.html#the-specificity-of-survey-data-1",
    "title": "Introduction to quantitative methods with ",
    "section": "The specificity of survey data",
    "text": "The specificity of survey data\n\nGranular nature: One observation is not a single individual; it represents a specific portion of the target population.\n Sampling weights: Essential to restore the representativeness of the target population.\nThe bias risk: Standard arithmetic functions (mean(), sd()) assume equal probability. On weighted data, they produce scientifically biased results.\nHorvitz-Thompson: Every observation \\(i\\) must be weighted by the inverse of its inclusion probability: \\(w_i = 1/\\pi_i\\). \\[\\hat{Y}_{HT} = \\sum_{s \\in S}\\frac{y_s}{\\pi_s}\\]"
  },
  {
    "objectID": "slides/stats.html#the-specificity-of-survey-data-2",
    "href": "slides/stats.html#the-specificity-of-survey-data-2",
    "title": "Introduction to quantitative methods with ",
    "section": "The specificity of survey data",
    "text": "The specificity of survey data\n\nThe weighting challenge: Using weights is not a default behavior in R; standard functions often fail to represent the distribution correctly.\nCDF Complexity: Granularity makes it difficult to estimate the Cumulative Distribution Function (CDF), which is the backbone of:\n\nQuantiles (Median, Deciles).\nInequality Indices (Gini, Atkinson)."
  },
  {
    "objectID": "slides/stats.html#point-estimates-with-hmisc",
    "href": "slides/stats.html#point-estimates-with-hmisc",
    "title": "Introduction to quantitative methods with ",
    "section": "Point Estimates with Hmisc",
    "text": "Point Estimates with Hmisc\nThe Hmisc package provides the simplest toolkit for calculating standard weighted indicators (Mean, Variance, Quantile, Rank).\n\nOptimized for dplyr: These functions are designed to fit perfectly within a summarise() pipeline.\nNaming Logic: Functions carry the same name as Base R, prefixed with wtd. (weighted).\nThe weights Argument: Unlike standard functions, you must explicitly pass your weight variable."
  },
  {
    "objectID": "slides/stats.html#at-least-two-ways-to-use-hmisc",
    "href": "slides/stats.html#at-least-two-ways-to-use-hmisc",
    "title": "Introduction to quantitative methods with ",
    "section": "At least two ways to use Hmisc",
    "text": "At least two ways to use Hmisc\nThe Base R approach (with)\nIdeal for quick, one-off calculations on a specific variable.\nlibrary(Hmisc)\n\n# Basic weighted statistics\nwith(t, wtd.mean(y, weights = p))     # Mean\nwith(t, wtd.std(y, weights = p))      # Standard Deviation\nwith(t, wtd.quantile(y, weights = p, probs = 0.5, type = 'quantile')) # Median\nThe Tidyverse approach (summarise)\nBest for producing comparative tables across different population groups.\n# Result: A tibble with the median for each category\nstat_table &lt;- t %&gt;%\n  group_by(categorie) %&gt;%\n  summarise(\n    mediane = wtd.quantile(y, weights = p, probs = 0.5, type = 'quantile')\n  )"
  },
  {
    "objectID": "slides/stats.html#estimating-value-and-variance",
    "href": "slides/stats.html#estimating-value-and-variance",
    "title": "Introduction to quantitative methods with ",
    "section": "Estimating value and variance",
    "text": "Estimating value and variance\nAny estimate based on survey data is surrounded by uncertainty. Sampling theory provides the mathematical framework to quantify this margin of error.\n\nThe challenge: To calculate the correct variance, one needs the point estimate and the details of the sampling design.\nThe tool: The survey package is the industry standard in R. It handles both simple and complex sampling plans."
  },
  {
    "objectID": "slides/stats.html#accessing-variance-information",
    "href": "slides/stats.html#accessing-variance-information",
    "title": "Introduction to quantitative methods with ",
    "section": "Accessing variance information",
    "text": "Accessing variance information\nIn practice, calculating the true variance can be difficult due to data availability:\n\nStandard approach: Requires variables for stratification and clustering. These are often confidential and removed from public datasets.\nReplicate weights approach: Some producers provide “replicate weights” (e.g., Bootstrap or Jackknife weights).\n\nExamples: Common in US data (SCF), or specific European/Insee surveys (HFCS, HVP).\nBenefit: Allows for valid variance estimation even without knowing the internal strata.\n\n\n\n\n\n\n\n\nWhy use the survey package?\n\n\nIt is specifically designed to handle both approaches. Whether you have the stratification variables or replicate weights, survey ensures your standard errors and confidence intervals are scientifically valid."
  },
  {
    "objectID": "slides/stats.html#national-production-and-advanced-variance",
    "href": "slides/stats.html#national-production-and-advanced-variance",
    "title": "Introduction to quantitative methods with ",
    "section": "National production and advanced variance",
    "text": "National production and advanced variance\nIn a professional or national statistical context, estimating variance must reflect the entire data processing chain, not just the initial sampling."
  },
  {
    "objectID": "slides/stats.html#the-challenge-of-real-world-variance",
    "href": "slides/stats.html#the-challenge-of-real-world-variance",
    "title": "Introduction to quantitative methods with ",
    "section": "The challenge of “real-world” variance",
    "text": "The challenge of “real-world” variance\nGeneral tools often overlook two critical steps that significantly impact the final precision of indicators:\n\nTotal non-response: The uncertainty added when adjusting weights to compensate for missing participants.\nCalibration (Calage): Re-weighting the sample to match known population totals (census), which mathematically alters the variance."
  },
  {
    "objectID": "slides/stats.html#the-case-of-france-gustave",
    "href": "slides/stats.html#the-case-of-france-gustave",
    "title": "Introduction to quantitative methods with ",
    "section": "The case of France: gustave",
    "text": "The case of France: gustave\nTo address these specificities, Insee (the French National Statistical Institute) developed the gustave package. It is now the internal standard for official French statistics.\n\nObjective: Account for the impact of non-response and calibration on precision.\nKey function: qvar(), which allows for variance estimation tailored to the sophisticated treatments applied to national surveys."
  },
  {
    "objectID": "slides/stats.html#best-practices-for-survey-analysis",
    "href": "slides/stats.html#best-practices-for-survey-analysis",
    "title": "Introduction to quantitative methods with ",
    "section": "Best practices for survey analysis",
    "text": "Best practices for survey analysis\nSurvey estimation is primarily about using weights correctly to reflect the national population. For most descriptive tasks, the Hmisc package is more than enough.\nHowever, if you need to calculate precision (e.g., to build confidence intervals for a report on poverty or health), you face two practical scenarios:\n\nScenario 1: Using replicate weights If your dataset includes replicate weights (which summarize the entire sampling process), the survey package is your best tool. This is common in many international datasets.\nScenario 2: Analytical variance estimation If you need to estimate variance analytically based on the specific sampling design (as is the standard for Insee surveys in France or complex national surveys), the gustave package is the most suitable choice."
  },
  {
    "objectID": "slides/stats.html#summary-of-recommendations",
    "href": "slides/stats.html#summary-of-recommendations",
    "title": "Introduction to quantitative methods with ",
    "section": "Summary of recommendations",
    "text": "Summary of recommendations\n\nKeep it simple for exploration: use Hmisc for quick weighted means and medians.\nBe rigorous for final results: always provide a confidence interval using survey or gustave.\nCheck your metadata: before starting, verify if your dataset provides “replicate weights” or “strata/cluster” variables to choose the right package.\n\n\n\nIntroduction to quantitative methods with , Lesotho Bureau of Statistics (back to main page)"
  },
  {
    "objectID": "slides/quarto.html#introduction",
    "href": "slides/quarto.html#introduction",
    "title": "Introduction to Quantitative Methods with ",
    "section": "Introduction",
    "text": "Introduction\n\n\n\n\n\n\nNote\n\n\n\nExercises associated with this chapter here\n\n\n\n\n\n\nBack to the main page"
  },
  {
    "objectID": "slides/quarto.html#challenges",
    "href": "slides/quarto.html#challenges",
    "title": "Introduction to Quantitative Methods with ",
    "section": "Challenges",
    "text": "Challenges\n\nProduce reproducible studies by integrating code and text in the same document\n\n\n\nThe complete generation of the study is contained in a single project\n\n\n\n\nLimit the risk of errors due to manual actions\n\n\n\n\nNative management of different formats for the final document (pdf, html, odt, etc.)"
  },
  {
    "objectID": "slides/quarto.html#r-markdown",
    "href": "slides/quarto.html#r-markdown",
    "title": "Introduction to Quantitative Methods with ",
    "section": "R Markdown",
    "text": "R Markdown\n\n\nR Markdown is an R package that allows linking\n\nText in Markdown format\nCode in R that can be executed and whose outputs can be integrated into the text\n\n\n\n\n\nSeparation of content and form of the document\n\n\n\nA document is compiled in two steps\n\nknit: the knitr package transforms the text and R outputs into a standard Markdown document\nconvert: the pandoc software transforms the .md document into a standard output format (html, pdf, etc.)"
  },
  {
    "objectID": "slides/quarto.html#quarto",
    "href": "slides/quarto.html#quarto",
    "title": "Introduction to Quantitative Methods with ",
    "section": "Quarto",
    "text": "Quarto\n\nQuarto is the successor to R Markdown\n\n\n\nQuarto supports different computation engines (knitr, Jupyter, Observable…) which makes it natively multi-language (R, Python, JavaScript…)\n\n\n\n\nThe functioning of both systems remains very similar"
  },
  {
    "objectID": "slides/quarto.html#anatomy-of-a-reproducible-document",
    "href": "slides/quarto.html#anatomy-of-a-reproducible-document",
    "title": "Introduction to Quantitative Methods with ",
    "section": "Anatomy of a Reproducible Document",
    "text": "Anatomy of a Reproducible Document"
  },
  {
    "objectID": "slides/quarto.html#additional-resources",
    "href": "slides/quarto.html#additional-resources",
    "title": "Introduction to Quantitative Methods with ",
    "section": "Additional Resources",
    "text": "Additional Resources\n\nThe official R Markdown documentation\nThe UtilitR guide on R Markdown\nThe official quarto documentation\n\n\n\nIntroduction to Quantitative Methods with , École Normale Supérieure (back to main page)"
  },
  {
    "objectID": "exercises/quarto.html",
    "href": "exercises/quarto.html",
    "title": "Reproducible report generation with  and Quarto",
    "section": "",
    "text": "One of the major challenges in research and data analysis is the transparent and reproducible communication of results, as well as collaboration. Reproducible publications offer a solution to this challenge by integrating code, data, and analyses into a format that allows anyone to understand the data processing steps that were implemented and to reproduce the exact results, provided they have the same environment.\nQuarto, the tool that succeeded to R Markdown, makes it possible to produce text documents while natively integrating chunks of R code. This helps streamline the writing process of a publication by greatly reducing the number of manual steps required to include graphics or code in a document: rather than having Stata code that generates Excel/Calc outputs which are then integrated into a Word document or transformed into LaTeX tables, we rely on a single source document that contains both the text and the code that produces the outputs of the final document.\nWith Quarto, we are able to separate content from presentation. This allows us to focus on the most important aspects: the substance of the analysis.\nAn example of such a formatted report is available on this page."
  },
  {
    "objectID": "exercises/quarto.html#initializing-the-document",
    "href": "exercises/quarto.html#initializing-the-document",
    "title": "Reproducible report generation with  and Quarto",
    "section": "1 Initializing the document",
    "text": "1 Initializing the document\n\n\n\n\n\n\nExercise 1: Creating the file\n\n\n\n\nClick on File &gt; New file &gt; Quarto document:\n\nProvide a title and an author for the report\nChoose HTML as the output format\nUncheck the Use visual markdown editor option\n\nSave the file under the name report.qmd.\nIn the file header, modify the publication metadata (title, author, date, etc.) using some of the options from this page.\nWrite a first sentence in the body of the document using one or two elements of Markdown syntax for formatting. For example, make part of the text bold.\nAdd a level-2 heading (two #) to create an About section and write one or two sentences in it.\nRender the document by clicking on the Render button:\n\n\n\n\nSource: Hello Quarto! by Posit\n\n\n\n\n\n\n\n\n\n\nExercise 2: Inserting R outputs\n\n\n\n\nChoose one of the chapters we previously worked on for which we produced a graph or a map. Reuse the example code from the solution (data import and visualizations) and insert it into the document.\nCompile the document. If there are errors, read them carefully and fix them in order to obtain a reproducible document.\nUse the GT package to create a nice table, for example a descriptive statistics table (a mean, for instance).\n\n\n\n\n\n\n\n\n\nExercise 3: Styling\n\n\n\nCustomize the output to obtain an aesthetically pleasing document. To do so, you may use a template.\n\n\n\n\n\n\n\n\nNote\n\n\n\nOn a Mac , some useful characters are more difficult to type. For example, | can be obtained using the following shortcut: Shift + Option + L"
  },
  {
    "objectID": "exercises/r-wrangling.html",
    "href": "exercises/r-wrangling.html",
    "title": "Importing and manipulating data with ",
    "section": "",
    "text": "In this second tutorial, we will learn to import and manipulate data with .\nIn this chapter, we will mainly use the following packages from the tidyverse:\nIn this tutorial, we will use two data sources:"
  },
  {
    "objectID": "exercises/r-wrangling.html#importing-a-csv-from-ademe",
    "href": "exercises/r-wrangling.html#importing-a-csv-from-ademe",
    "title": "Importing and manipulating data with ",
    "section": "2.1 Importing a csv from Ademe",
    "text": "2.1 Importing a csv from Ademe\nTo start, we’ll import Ademe data using the readr package[^readcsv].\n\n\n\n\n\n\nExercise 1: reading a csv with readr and observing the data\n\n\n\nHere’s the URL where the data is available\n\nurl &lt;- \"https://koumoul.com/s/data-fair/api/v1/datasets/igt-pouvoir-de-rechauffement-global/convert\"\n\n\nUse the readr package to import this data. Name this object emissions4\n\n\n\nSolution\nlibrary(readr)\nemissions &lt;- read_csv(url)\n\n\n\nDisplay the first lines with head and observe the display difference with, for example, this dataframe:\n\n\ndf &lt;- data.frame(\n  var1 = 1:10,\n  var2 = letters[1:10],\n  var3 = rep(c(TRUE, FALSE), times = 5)\n)\n\n\n\nSolution\nhead(emissions)\nhead(df)\n\n\n\nDisplay the class of emissions. Do you now understand why this object is slightly different from a base dataframe?\n\n\n\nSolution\nclass(emissions)\n\n\n\nUse appropriate functions for the first 10 values, the last 15 and a random sample of 10 values using the appropriate function from the dplyr package\n\n\n\nIf stuck on question 1\n\nRead the read_csv documentation (very well done) or search for examples online to discover this function. ⚠️ Don’t use read.csv (base function) which isn’t performant."
  },
  {
    "objectID": "exercises/r-wrangling.html#first-data-manipulations",
    "href": "exercises/r-wrangling.html#first-data-manipulations",
    "title": "Importing and manipulating data with ",
    "section": "2.2 First data manipulations",
    "text": "2.2 First data manipulations\nAs mentioned in utilitR, the main functions of dplyr (the verbs of the dplyr grammar) are as follows:\n\nselect(): select variables by their name;\nrename(): rename variables;\nfilter(): select observations according to one or more conditions;\narrange(): sort the table according to one or more variables;\nmutate(): add variables that are functions of other variables;\nsummarise(): calculate a statistic from data;\ngroup_by(): perform operations by group.\n\n\nviewof dplyrVerbs = Inputs.select(['select','rename','filter','mutate', 'arrange'], {value: \"select\"})\n\n\n\n\n\n\n\n\n\nhtml`&lt;img src=\"https://github.com/linogaliana/r-geographie/raw/main/exercises/img/${dplyrVerbs}.png\" width=\"60%\"&lt;/&gt;`\n\n\n\n\n\n\nFigure 1: Illustration of dplyr verbs\n\n\nThe following cheatsheet is very practical as it illustrates these different functions. It’s recommended to regularly consult it (click on the image to zoom 🔎):\n\n\n\n\n\n\n\n\n\n\n\ndplyr Cheatsheets\n\n\n\n\n\n\n\n\n\nExercise 2: discovering dplyr verbs for data manipulation\n\n\n\nFirst, let’s familiarize ourselves with operations on columns.\n\nCreate a dataframe emissions_copy keeping only the columns INSEE commune, Commune, Autres transports and Autres transports international\n\n\n\nHint for this question\n\n\n\n\nSince variable names are impractical, rename them as follows:\n\nINSEE commune \\(\\to\\) code_insee\nAutres transports \\(\\to\\) transports\nAutres transports international \\(\\to\\) transports_international\n\n\n\n\nHint for this question\n\n\n\n\nTo simplify, let’s replace missing values (NA) with the value 05. Use the replace_na function from the tidyr package, in conjunction with mutate, to transform missing values to 0.\n\n\n\nHint for this question\n\n\n\n\nCreate the following variables in the same code sequence:\n\ndep: the department. This can be created using the first two characters of code_insee with the str_sub function from the stringr package6\ntransports_total: transport sector emissions (sum of the two variables)\n\n\n\n\nHint for this question\n\n\n\n\nOrder the data from highest to lowest polluter then order the data from highest to lowest polluter by department (from 01 to 95).\n\n\n\nHint for this question\n\n\n\n\nCalculate total emissions by department\n\n\n\nHint for this question\n\n\n“Group by” = group_by\n“total emissions” = summarise(sum(***))\n\n\n\n\n\n\nSolution question 1\nemissions_copy &lt;- emissions %&gt;%\n  select(`INSEE commune`, `Commune`, `Autres transports`, `Autres transports international`)\n\n\n\n\nSolution question 2\nemissions_copy &lt;- emissions_copy %&gt;%\n  rename(\n    code_insee = `INSEE commune`,\n    transports = `Autres transports`,\n    transports_international = `Autres transports international`\n    )\n\n\n\n\nSolution question 3\nemissions_copy &lt;- emissions_copy %&gt;%\n  mutate(\n    transports = replace_na(transports),\n    transports_international = replace_na(transports_international)\n    )\n\n\n\n\nSolution question 4\nemissions_copy &lt;- emissions_copy %&gt;%\n  mutate(\n    dep = str_sub(code_insee, 1, 2),\n    transports_total = transports + transports_international\n    )\n\n\n\n\nSolution question 5\nemissions_copy %&gt;%\n  arrange(desc(transports_total))\n\nemissions_copy %&gt;%\n  arrange(dep, desc(transports_total))\n\n\n\n\nSolution question 6\nemissions_copy %&gt;%\n  group_by(dep) %&gt;%\n  summarise(sum(transports_total))"
  },
  {
    "objectID": "exercises/r-wrangling.html#importing-insee-data",
    "href": "exercises/r-wrangling.html#importing-insee-data",
    "title": "Importing and manipulating data with ",
    "section": "2.3 Importing Insee data",
    "text": "2.3 Importing Insee data\nFor our municipal information, we’ll use one of Insee’s most used sources: Filosofi data. To facilitate retrieving these, we’ll use the community package doremifasol:\n\nlibrary(doremifasol)\nlibrary(tibble)\nfilosofi &lt;- as_tibble(\n  telechargerDonnees(\"FILOSOFI_COM\", date = 2016)\n)\nhead(filosofi)\n\n\n\n# A tibble: 6 × 29\n  CODGEO LIBGEO      NBMENFISC16 NBPERSMENFISC16  MED16 PIMP16 TP6016 TP60AGE116\n  &lt;chr&gt;  &lt;chr&gt;             &lt;dbl&gt;           &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;      &lt;dbl&gt;\n1 01001  L'Abergeme…         313            796. 22679      NA     NA         NA\n2 01002  L'Abergeme…         101            248  24382.     NA     NA         NA\n3 01004  Ambérieu-e…        6363          14228  19721      49     17         19\n4 01005  Ambérieux-…         633           1662. 23378      NA     NA         NA\n5 01006  Ambléon              NA             NA     NA      NA     NA         NA\n6 01007  Ambronay           1087           2684  22146.     57     NA         NA\n# ℹ 21 more variables: TP60AGE216 &lt;dbl&gt;, TP60AGE316 &lt;dbl&gt;, TP60AGE416 &lt;dbl&gt;,\n#   TP60AGE516 &lt;dbl&gt;, TP60AGE616 &lt;dbl&gt;, TP60TOL116 &lt;dbl&gt;, TP60TOL216 &lt;dbl&gt;,\n#   PACT16 &lt;dbl&gt;, PTSA16 &lt;dbl&gt;, PCHO16 &lt;dbl&gt;, PBEN16 &lt;dbl&gt;, PPEN16 &lt;dbl&gt;,\n#   PPAT16 &lt;dbl&gt;, PPSOC16 &lt;dbl&gt;, PPFAM16 &lt;dbl&gt;, PPMINI16 &lt;dbl&gt;, PPLOGT16 &lt;dbl&gt;,\n#   PIMPOT16 &lt;dbl&gt;, D116 &lt;dbl&gt;, D916 &lt;dbl&gt;, RD16 &lt;dbl&gt;\n\n\n\n\n\n\n\n\nNote\n\n\n\nThe as_tibble function converts the base dataframe (doremifasol doesn’t make assumptions about the manipulation ecosystem adopted) into a dataframe adapted for use via the tidyverse."
  },
  {
    "objectID": "exercises/r-wrangling.html#footnotes",
    "href": "exercises/r-wrangling.html#footnotes",
    "title": "Importing and manipulating data with ",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nreadr offers the possibility to import data directly from a url. This is the option taken in this tutorial. If you prefer, for network access or performance reasons, to import from a local machine, you can download the data and change the import commands with the appropriate path instead of the url.↩︎\nThere’s also bioconductor but since it’s mainly oriented towards biostatistics (one of the academic communities that adopted  earliest), we don’t really use it↩︎\nremotes::install_github means to use the install_github function from the package remotes. In other words, you need a package to install other packages 🤯. This is because Github didn’t exist when  was created (1990s) and this functionality hasn’t been added since.↩︎\nFor lack of imagination, we’re often tempted to call our main dataframe df or data. This is often a bad idea since this name isn’t very informative when rereading the code a few weeks later. Self-documentation, an approach that consists of having code that explains itself, is a good practice and it’s therefore recommended to give a simple but effective name to know the nature of the dataset in question.↩︎\nThis assumption is certainly false. It’s exclusively here to illustrate variable creation via mutate.↩︎\nTo be really precise, we would need to modify the values obtained for Corsican departments with the case_when function from the dplyr package. This is left as an additional exercise.↩︎\nThe space in the variable name is annoying. To be able to use this variable’s name in rename, we’ll need to use backticks, i.e., INSEE commune.↩︎\nThe limited functionalities of the base language for text manipulation quickly become constraining. We thus quickly move to stringr even though it’s not the main subject of the chapter.↩︎\nyou can directly use the helper code snippet if you’re not familiar with ggplot↩︎\nIdeally, we should ensure this join doesn’t introduce bias. Indeed, since our reference years aren’t necessarily identical, there may be a mismatch between our two sources. Since the tutorial is already long, we won’t go down this path. Interested readers can perform such an analysis as an additional exercise.↩︎"
  }
]